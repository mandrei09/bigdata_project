{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe6e0aa",
   "metadata": {},
   "source": [
    "# 1. Introducere\n",
    "\n",
    "## Prezentarea succintă a setului de date\n",
    "\n",
    "Setul de date utilizat este **Heart Failure Prediction**, disponibil pe Kaggle:\n",
    "https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction\n",
    "\n",
    "Setul de date conține 918 înregistrări despre pacienți, fiecare cu 12 caracteristici medicale și o variabilă țintă `HeartDisease` care indică dacă pacientul **are sau nu boală cardiovasculară**.\n",
    "\n",
    "## Obiective\n",
    "\n",
    "Obiectivul proiectului este:\n",
    "- **procesarea**, **curățarea** și **analizarea** setului de date folosind *Spark*;\n",
    "- **antrenarea** și **evaluarea** a două modele *ML*;\n",
    "- **aplicarea** unei metode de *DL* cu *TensorFlow*;\n",
    "- **implementarea** unui *pipeline* și *UDF*;\n",
    "- **integrarea** unui *flux de date în timp real* cu *Spark Streaming*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19630cf",
   "metadata": {},
   "source": [
    "# 2. Procesarea datelor cu Spark\n",
    "\n",
    "Vom folosi *PySpark* pentru a **analiza** și **prelucra** datele. Se vor aplica **agregări** și **transformări** folosind atât *DataFrame API* cât și *Spark SQL*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7a2ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/15 09:56:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- ChestPainType: string (nullable = true)\n",
      " |-- RestingBP: integer (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- FastingBS: integer (nullable = true)\n",
      " |-- RestingECG: string (nullable = true)\n",
      " |-- MaxHR: integer (nullable = true)\n",
      " |-- ExerciseAngina: string (nullable = true)\n",
      " |-- Oldpeak: double (nullable = true)\n",
      " |-- ST_Slope: string (nullable = true)\n",
      " |-- HeartDisease: integer (nullable = true)\n",
      "\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "| 40|  M|          ATA|      140|        289|        0|    Normal|  172|             N|    0.0|      Up|           0|\n",
      "| 49|  F|          NAP|      160|        180|        0|    Normal|  156|             N|    1.0|    Flat|           1|\n",
      "| 37|  M|          ATA|      130|        283|        0|        ST|   98|             N|    0.0|      Up|           0|\n",
      "| 48|  F|          ASY|      138|        214|        0|    Normal|  108|             Y|    1.5|    Flat|           1|\n",
      "| 54|  M|          NAP|      150|        195|        0|    Normal|  122|             N|    0.0|      Up|           0|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HeartFailurePrediction\").getOrCreate()\n",
    "\n",
    "# Citirea setului de date\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"heart.csv\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "205b5459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/15 09:56:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "|summary|               Age| Sex|ChestPainType|         RestingBP|       Cholesterol|          FastingBS|RestingECG|             MaxHR|ExerciseAngina|           Oldpeak|ST_Slope|       HeartDisease|\n",
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "|  count|               918| 918|          918|               918|               918|                918|       918|               918|           918|               918|     918|                918|\n",
      "|   mean|53.510893246187365|NULL|         NULL|132.39651416122004| 198.7995642701525|0.23311546840958605|      NULL|136.80936819172112|          NULL|0.8873638344226581|    NULL| 0.5533769063180828|\n",
      "| stddev|  9.43261650673202|NULL|         NULL|18.514154119907808|109.38414455220345|0.42304562473930296|      NULL| 25.46033413825029|          NULL|1.0665701510493264|    NULL|0.49741373828459706|\n",
      "|    min|                28|   F|          ASY|                 0|                 0|                  0|       LVH|                60|             N|              -2.6|    Down|                  0|\n",
      "|    max|                77|   M|           TA|               200|               603|                  1|        ST|               202|             Y|               6.2|      Up|                  1|\n",
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "\n",
      "+------------+-----+\n",
      "|HeartDisease|count|\n",
      "+------------+-----+\n",
      "|           1|  508|\n",
      "|           0|  410|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistici descriptive\n",
    "df.describe().show()\n",
    "\n",
    "# Distribuția țintei (HeartDisease)\n",
    "df.groupBy(\"HeartDisease\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec757d",
   "metadata": {},
   "source": [
    "## Curățare și transformare date\n",
    "\n",
    "**Verificăm** *valori lipsă* și **realizăm transformări** simple: *conversia la lowercase*, *eliminarea duplicatelor*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfacaca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|  0|  0|            0|        0|          0|        0|         0|    0|             0|      0|       0|           0|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificare valori lipsă\n",
    "from pyspark.sql.functions import col, isnan, when, count, trim\n",
    "\n",
    "# Pentru fiecare coloana din set-ul nostru de date, numaram valorile egale cu NULL sau stringurile vide.\n",
    "df.select([\n",
    "    count(\n",
    "        when(\n",
    "            col(c).isNull() | (trim(col(c)) == \"\"), c\n",
    "        )\n",
    "    ).alias(c)\n",
    "    for c in df.columns\n",
    "]).show()\n",
    "\n",
    "# Eliminare duplicate (dacă există)\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c0cf9",
   "metadata": {},
   "source": [
    "Din fericire set-ul de date pe care l-am ales **nu a continut** *NULL* sau *stringuri vide*, dar acest lucru **nu este valabil** pentru orice dataset, deci acesta prelucrare **este obligatorie**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39a424",
   "metadata": {},
   "source": [
    "## Grupări și agregări cu DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e72a9",
   "metadata": {},
   "source": [
    "Agregare: procentul bolnavilor pe gen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0df7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|Sex|HeartDiseaseRatePercent|\n",
      "+---+-----------------------+\n",
      "|  F|                  25.91|\n",
      "|  M|                  63.17|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round, avg\n",
    "\n",
    "df.groupBy(\"Sex\") \\\n",
    "  .agg(round(avg(\"HeartDisease\") * 100, 2).alias(\"HeartDiseaseRatePercent\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d874e34",
   "metadata": {},
   "source": [
    "Grupare pe categorii de vârstă:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8fd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|AgeGroup|HeartDiseaseRatePercent|\n",
      "+--------+-----------------------+\n",
      "|   40-59|                  50.77|\n",
      "|     60+|                  73.12|\n",
      "|     <40|                   32.5|\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn(\"AgeGroup\", when(col(\"Age\") < 40, \"<40\")\n",
    "                              .when((col(\"Age\") >= 40) & (col(\"Age\") < 60), \"40-59\")\n",
    "                              .otherwise(\"60+\"))\n",
    "\n",
    "df.groupBy(\"AgeGroup\") \\\n",
    "  .agg(round(avg(\"HeartDisease\") * 100, 2).alias(\"HeartDiseaseRatePercent\")) \\\n",
    "  .orderBy(\"AgeGroup\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2192ffd",
   "metadata": {},
   "source": [
    "## Interogări cu Spark SQL\n",
    "\n",
    "Rata bolii în funcție de *glicemie* (**FastingBS**) și *tensiune arterială* (**RestingBP**):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e79c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----------------------+\n",
      "|Diabetic|HighBP|Total|HeartDiseaseRatePercent|\n",
      "+--------+------+-----+-----------------------+\n",
      "|       1|     0|   80|                   80.0|\n",
      "|       1|     1|  134|                   79.1|\n",
      "|       0|     1|  409|                  50.86|\n",
      "|       0|     0|  295|                  44.07|\n",
      "+--------+------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"HighBP\", when(col(\"RestingBP\") >= 130, 1).otherwise(0))\n",
    "df.createOrReplaceTempView(\"heart\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "       FastingBS AS Diabetic,\n",
    "       HighBP,\n",
    "       COUNT(*) AS Total,\n",
    "       ROUND(AVG(HeartDisease) * 100, 2) AS HeartDiseaseRatePercent\n",
    "    FROM heart\n",
    "    GROUP BY FastingBS, HighBP\n",
    "    ORDER BY HeartDiseaseRatePercent DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62bf99",
   "metadata": {},
   "source": [
    "# 3. Metode de Machine Learning\n",
    "\n",
    "Vom **construi** și **evalua** *două modele* de *clasificare binară* pentru a **prezice** apariția *bolii cardiace (`HeartDisease`)*:\n",
    "\n",
    "1. *Logistic Regression*\n",
    "2. *Random Forest Classifier*\n",
    "\n",
    "Scopul este să comparăm performanța celor două metode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4417276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+-----+\n",
      "|features                                            |label|\n",
      "+----------------------------------------------------+-----+\n",
      "|(11,[0,1,2,4,7,10],[40.0,140.0,289.0,172.0,2.0,1.0])|1.0  |\n",
      "|[49.0,160.0,180.0,0.0,156.0,1.0,1.0,1.0,0.0,0.0,0.0]|0.0  |\n",
      "|[37.0,130.0,283.0,0.0,98.0,0.0,0.0,2.0,2.0,0.0,1.0] |1.0  |\n",
      "|[48.0,138.0,214.0,0.0,108.0,1.5,1.0,0.0,0.0,1.0,0.0]|0.0  |\n",
      "|(11,[0,1,2,4,7,10],[54.0,150.0,195.0,122.0,1.0,1.0])|1.0  |\n",
      "+----------------------------------------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Selectăm coloanele numerice\n",
    "feature_cols = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "# Transformăm și coloanele categorice în numeric (dacă există)\n",
    "categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\") for col in categorical_cols]\n",
    "\n",
    "# Asamblare vector caracteristici\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols + [col + \"_idx\" for col in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Etichetă (HeartDisease este deja 0/1 dar asigurăm consistența)\n",
    "label_indexer = StringIndexer(inputCol=\"HeartDisease\", outputCol=\"label\")\n",
    "\n",
    "# Pipeline de preprocesare\n",
    "pipeline = Pipeline(stages=indexers + [assembler, label_indexer])\n",
    "data = pipeline.fit(df).transform(df).select(\"features\", \"label\")\n",
    "data.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd804f8",
   "metadata": {},
   "source": [
    "## Împărțirea datelor\n",
    "\n",
    "Vom împărți setul de date în două subseturi: antrenare (70%) și test (30%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a93d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 681, Test: 237\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "print(f\"Train: {train_data.count()}, Test: {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a80236",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "- Vrem să **prezicem** dacă o persoană are *boală cardiacă* (*HeartDisease = 1*) pe baza unor *caracteristici clinice*.\n",
    "\n",
    "- **Logistic Regression** este o alegere bună pentru *clasificare binară*, deoarece este *interpretabil*, *rapid de antrenat* și oferă o primă linie de bază.\n",
    "\n",
    "- **Aplicăm** *modelul* și **evaluăm** *acuratețea* pe *setul de testare*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (Logistic Regression): 0.8969\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "lr_auc = evaluator.evaluate(lr_predictions)\n",
    "print(f\"AUC (Logistic Regression): {lr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9517a5",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "La fel ca mai sus, dorim să facem clasificare binară.\n",
    "\n",
    "Random Forest este un model de tip \"ensemble\" ce poate surprinde relații non-liniare între variabile. Este mai robust decât Logistic Regression, dar mai lent.\n",
    "\n",
    "Antrenăm un model RF și comparăm performanța sa cu Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f43b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (Random Forest): 0.9155\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, seed=42)\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "rf_auc = evaluator.evaluate(rf_predictions)\n",
    "print(f\"AUC (Random Forest): {rf_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44521f75",
   "metadata": {},
   "source": [
    "## Concluzii ML\n",
    "\n",
    "Pentru ambele modele am folosit *AUC* - (**Area Under Curve**). \n",
    "\n",
    "Aceasta este o *metrică standard* folosită pentru *evaluarea performanței* unui model de *clasificare binară*, în special când clasele sunt *dezechilibrate*.\n",
    "\n",
    "| Model               | AUC         |\n",
    "|--------------------|-------------|\n",
    "| *Logistic Regression*| ~ *0.8969* |\n",
    "| *Random Forest*      | ~ *0.9155* |\n",
    "\n",
    "*Random Forest* a oferit performanță mai bună datorită capacității sale de a **modela relații complexe** între trăsături. Totuși, *Logistic Regression* este **util** ca *model de bază* și pentru *interpretabilitate*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519008d",
   "metadata": {},
   "source": [
    "## 4. Utilizarea unui Data Pipeline complet\n",
    "\n",
    "Vom **construi** un *Pipeline* care include:\n",
    "\n",
    "- **Preprocesare**: *indexare categorice* + *VectorAssembler*\n",
    "- **Model ML**: *Logistic Regression*\n",
    "\n",
    "Ne dorim să automatizăm întreaga secvență de transformări și învățare automată.\n",
    "\n",
    "Etape: \n",
    "\n",
    "1. **Indexare categorică**: Coloanele *string* sunt transformate în coloane numerice folosind *StringIndexer*.\n",
    "\n",
    "2. **VectorAssembler**: Toate coloanele numerice și cele indexate sunt combinate într-un singur vector de trăsături (*features*) necesar pentru modele ML.\n",
    "\n",
    "3. **Label Indexing**: Coloana țintă *HeartDisease* este transformată în eticheta *label*.\n",
    "\n",
    "4. **Modelul**: Se utilizează *LogisticRegression* pentru clasificare binară.\n",
    "\n",
    "5. **Pipeline**: Toți pașii anteriori sunt legați într-un *Pipeline* unitar ce permite aplicarea secvențială a etapelor.\n",
    "\n",
    "6. **Antrenare & Predicție**: *fit()* antrenează pipeline-ul pe date, *transform()* aplică modelul pe același set.\n",
    "\n",
    "7. **Evaluare**: Se utilizează *BinaryClassificationEvaluator* cu metrica *areaUnderROC* pentru a evalua performanța modelului."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b8b8156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/15 09:57:34 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC cu pipeline complet (Logistic Regression): 0.9194\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Refacem pipeline-ul complet: indexeri + assembler + model\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\") for col in categorical_cols]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols + [col + \"_idx\" for col in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "label_indexer = StringIndexer(inputCol=\"HeartDisease\", outputCol=\"label\")\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Pipeline complet\n",
    "full_pipeline = Pipeline(stages=indexers + [assembler, label_indexer, lr])\n",
    "\n",
    "# Antrenare + predicție\n",
    "pipeline_model = full_pipeline.fit(df)\n",
    "predictions = pipeline_model.transform(df)\n",
    "\n",
    "# Evaluare rapidă\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "pipeline_auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC cu pipeline complet (Logistic Regression): {pipeline_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e917c",
   "metadata": {},
   "source": [
    "## 5. Utilizarea unei funcții definite de utilizator (UDF)\n",
    "\n",
    "Voi crea o funcție care calculează un scor de risc personalizat în funcție de vârstă și colesterol, urmând să adaug acest scor ca o coloană nouă pentru analiză sau input în model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "677270ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+\n",
      "|Age|Cholesterol|RiskScore|\n",
      "+---+-----------+---------+\n",
      "| 40|        289|   0.8225|\n",
      "| 49|        180|   0.5725|\n",
      "| 37|        283|      0.8|\n",
      "| 48|        214|    0.655|\n",
      "| 54|        195|   0.6225|\n",
      "+---+-----------+---------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Funcție de calcul risc: rudimentară pentru demonstrație\n",
    "def risk_score(age, cholesterol):\n",
    "    score = 0.5 * age + 0.5 * (cholesterol if cholesterol else 0)\n",
    "    return float(score / 200)  # normalizare\n",
    "\n",
    "risk_udf = udf(risk_score, DoubleType())\n",
    "\n",
    "# Aplicare UDF\n",
    "df_with_risk = df.withColumn(\"RiskScore\", risk_udf(col(\"Age\"), col(\"Cholesterol\")))\n",
    "df_with_risk.select(\"Age\", \"Cholesterol\", \"RiskScore\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127762d8",
   "metadata": {},
   "source": [
    "## Optimizarea hiperparametrilor (Grid Search)\n",
    "\n",
    "Voi **aplica** un *Grid Search* și *Cross Validation* pentru *Logistic Regression*.\n",
    "\n",
    "Îmi doresc să optimizez hiperparametrii, astfel, trebuie să găsesc valoarea optimă pentru *regParam* și *elasticNetParam*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7802870",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/multiprocessing/pool.py:856\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     item = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_items\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[31mIndexError\u001b[39m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     14\u001b[39m crossval = CrossValidator(estimator=pipeline_lr,\n\u001b[32m     15\u001b[39m                           estimatorParamMaps=param_grid,\n\u001b[32m     16\u001b[39m                           evaluator=evaluator,\n\u001b[32m     17\u001b[39m                           numFolds=\u001b[32m3\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Execută căutarea\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m cv_model = \u001b[43mcrossval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m best_model = cv_model.bestModel\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Afișăm AUC pe modelul optimizat\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/tuning.py:858\u001b[39m, in \u001b[36mCrossValidator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    852\u001b[39m train = datasets[i][\u001b[32m0\u001b[39m].cache()\n\u001b[32m    854\u001b[39m tasks = \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m    855\u001b[39m     inheritable_thread_target(dataset.sparkSession),\n\u001b[32m    856\u001b[39m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[32m    857\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollectSubModelsParam\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/multiprocessing/pool.py:861\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28mself\u001b[39m._pool = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    863\u001b[39m     item = \u001b[38;5;28mself\u001b[39m._items.popleft()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Pipeline de bază (doar cu Logistic Regression)\n",
    "pipeline_lr = Pipeline(stages=indexers + [assembler, label_indexer, lr])\n",
    "\n",
    "# Grid de parametri\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Execută căutarea\n",
    "cv_model = crossval.fit(df)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Afișăm AUC pe modelul optimizat\n",
    "cv_auc = evaluator.evaluate(best_model.transform(df))\n",
    "print(f\"AUC cu Logistic Regression + param tuning: {cv_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda3159",
   "metadata": {},
   "source": [
    "## Concluzii\n",
    "\n",
    "Am **integrat** într-un *pipeline complet* pașii de *preprocesare*, *antrenare* și *evaluare*. Am **utilizat**:\n",
    "\n",
    "- *UDF* pentru **definirea** unei *logici personalizate de scor de risc*\n",
    "- *Grid search* și *cross validation* pentru *optimizarea hiperparametrilor*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ad205",
   "metadata": {},
   "source": [
    "# 6. Deep Learning cu TensorFlow\n",
    "\n",
    "## Problemă\n",
    "\n",
    "Dorim să construim un model de clasificare binară care prezice apariția unei boli cardiace (`HeartDisease = 0 sau 1`) folosind rețele neuronale.\n",
    "\n",
    "## Justificare\n",
    "\n",
    "Rețelele neuronale artificiale pot capta relații complexe, neliniare între variabile, mai ales când există mai multe atribute implicate. În comparație cu Logistic Regression sau Random Forest, DL oferă flexibilitate mai mare pentru modelarea relațiilor nelineare.\n",
    "\n",
    "## Soluție\n",
    "\n",
    "Vom folosi TensorFlow + Keras pentru a antrena o rețea neuronală simplă cu:\n",
    "\n",
    "- 2 straturi ascunse (dense)\n",
    "- Funcția de activare `ReLU`\n",
    "- Funcția de pierdere `binary_crossentropy`\n",
    "- Optimizator `Adam`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43d4abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Salvăm datele Spark într-un Pandas DataFrame pentru TensorFlow\n",
    "pandas_df = df.select(\n",
    "    \"Age\", \"Sex\", \"ChestPainType\", \"RestingBP\", \"Cholesterol\",\n",
    "    \"FastingBS\", \"RestingECG\", \"MaxHR\", \"ExerciseAngina\", \n",
    "    \"Oldpeak\", \"ST_Slope\", \"HeartDisease\"\n",
    ").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72493f",
   "metadata": {},
   "source": [
    "## Preprocesare date pentru TensorFlow\n",
    "\n",
    "Vom converti datele categorice în one-hot encoding și vom scala datele numerice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2804f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Separăm features și label\n",
    "X = pandas_df.drop(\"HeartDisease\", axis=1)\n",
    "y = pandas_df[\"HeartDisease\"]\n",
    "\n",
    "# Coloane categorice și numerice\n",
    "cat_cols = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"]\n",
    "num_cols = [\"Age\", \"RestingBP\", \"Cholesterol\", \"FastingBS\", \"MaxHR\", \"Oldpeak\"]\n",
    "\n",
    "# Pipeline de transformare\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(), cat_cols)\n",
    "])\n",
    "\n",
    "# Aplicăm transformările\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Împărțire în train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a424e0",
   "metadata": {},
   "source": [
    "## Construirea modelului TensorFlow\n",
    "\n",
    "Model secvențial cu 2 straturi ascunse și un strat final cu sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d383f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 09:58:04.517672: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-15 09:58:04.518433: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-15 09:58:04.523461: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-15 09:58:04.532728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749981484.549408     952 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749981484.554505     952 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749981484.568485     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749981484.568501     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749981484.568502     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749981484.568503     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-15 09:58:04.572540: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-06-15 09:58:05.888882: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,217</span> (4.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,217\u001b[0m (4.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,217</span> (4.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,217\u001b[0m (4.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # output pentru clasificare binară\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7104ec",
   "metadata": {},
   "source": [
    "## Antrenarea modelului\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c6f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77382587",
   "metadata": {},
   "source": [
    "## Evaluarea modelului pe setul de testare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8468 - loss: 0.3631 \n",
      "Test Accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23991ca7",
   "metadata": {},
   "source": [
    "## Concluzie Deep Learning\n",
    "\n",
    "În această secțiune am folosit **TensorFlow** pentru a construi și antrena un model de rețea neuronală care prezice probabilitatea de boală cardiacă pe baza caracteristicilor pacienților. Pașii principali sunt:\n",
    "\n",
    "1. **Conversie Spark → Pandas**:\n",
    "   - Datele Spark sunt convertite în format Pandas (`toPandas()`) pentru a fi compatibile cu TensorFlow și Scikit-learn.\n",
    "\n",
    "2. **Preprocesare date**:\n",
    "   - Separăm `X` (features) și `y` (eticheta).\n",
    "   - Coloanele numerice sunt scalate (`StandardScaler`) iar cele categorice sunt codificate (`OneHotEncoder`) cu un `ColumnTransformer`.\n",
    "\n",
    "3. **Împărțirea datasetului**:\n",
    "   - Setul este împărțit în subseturi de antrenare și testare (`train_test_split`), cu 80% pentru antrenare și 20% pentru test.\n",
    "\n",
    "4. **Crearea modelului**:\n",
    "   - Rețea neuronală secvențială cu 2 straturi ascunse:\n",
    "     - 32 neuroni → 16 neuroni → 1 neuron de ieșire cu activare `sigmoid` (clasificare binară).\n",
    "   - Optimizator: `adam`, funcție de pierdere: `binary_crossentropy`.\n",
    "\n",
    "5. **Antrenare și evaluare**:\n",
    "   - Modelul este antrenat pe 80% din datele de antrenare, cu validare internă de 20%.\n",
    "   - Evaluarea se face pe setul de test (`evaluate()`), iar acuratețea este afișată.\n",
    "\n",
    "Modelul de *rețea neuronală* **oferă** o *acuratețe competitivă* pe *setul de testare*. \n",
    "\n",
    "`Avantaje`:\n",
    "- **Capacitate de modelare** *complexă*\n",
    "- **Poate învăța** *relații subtile* în date\n",
    "\n",
    "`Dezavantaje`:\n",
    "- **Necesită** *mai multă putere computațională*\n",
    "- **Mai puțin interpretabil** decât modele simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad6ae8",
   "metadata": {},
   "source": [
    "# 7. Streaming în Spark cu inferență ML în timp real\n",
    "\n",
    "## Problemă\n",
    "\n",
    "Simulăm un flux de date (stream) de la un fișier CSV care este completat treptat. Vom folosi `Spark Structured Streaming` pentru a procesa în timp real și a aplica **modelul ML Logistic Regression antrenat anterior** pentru a face inferență pe fiecare lot de date nou sosite.\n",
    "\n",
    "## Justificare\n",
    "\n",
    "În aplicații reale precum monitorizarea pacienților, datele vin în timp real. Spark Structured Streaming ne permite să facem inferență în timp real la scară mare.\n",
    "\n",
    "## Soluție\n",
    "\n",
    "1. Folosim un director monitorizat ca sursă de date (CSV-uri simulate incremental).\n",
    "2. Aplicăm același pipeline de preprocesare și predicție deja antrenat.\n",
    "3. Afișăm rezultatele în timp real în consolă.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c88d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Definim schema explicită (pentru streaming)\n",
    "schema = StructType() \\\n",
    "    .add(\"Age\", IntegerType()) \\\n",
    "    .add(\"Sex\", StringType()) \\\n",
    "    .add(\"ChestPainType\", StringType()) \\\n",
    "    .add(\"RestingBP\", IntegerType()) \\\n",
    "    .add(\"Cholesterol\", IntegerType()) \\\n",
    "    .add(\"FastingBS\", IntegerType()) \\\n",
    "    .add(\"RestingECG\", StringType()) \\\n",
    "    .add(\"MaxHR\", IntegerType()) \\\n",
    "    .add(\"ExerciseAngina\", StringType()) \\\n",
    "    .add(\"Oldpeak\", DoubleType()) \\\n",
    "    .add(\"ST_Slope\", StringType()) \\\n",
    "    .add(\"HeartDisease\", IntegerType())  # eticheta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a034a",
   "metadata": {},
   "source": [
    "## Inițializare flux streaming dintr-un director\n",
    "\n",
    "Simulăm fluxul adăugând incremental fișiere CSV în directorul `stream_input/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaec338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citim streaming dintr-un director local (CSV)\n",
    "stream_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .csv(\"stream_input/\")  # creezi acest director și pui fișierele treptat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411a349",
   "metadata": {},
   "source": [
    "## Aplicarea modelului Logistic Regression antrenat anterior pe fiecare micro-batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folosim pipeline-ul deja antrenat (de la punctul anterior)\n",
    "# pipeline_model este un model de tip PipelineModel cu LogisticRegression\n",
    "\n",
    "# Aplicăm transformări și predicții pe fluxul de date\n",
    "predictions_stream = pipeline_model.transform(stream_df)\n",
    "\n",
    "# Selectăm doar coloanele relevante\n",
    "results = predictions_stream.select(\"Age\", \"Sex\", \"Cholesterol\", \"prediction\", \"probability\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140c42b",
   "metadata": {},
   "source": [
    "## Scriem rezultatele în consolă în mod continuu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdbb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = results.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7446df",
   "metadata": {},
   "source": [
    "## Concluzie\n",
    "\n",
    "Am implementat un flux de streaming Spark care:\n",
    "\n",
    "- Monitorizează fișiere CSV în timp real\n",
    "- Aplică un model ML (Logistic Regression) pe fiecare batch de date\n",
    "- Returnează predicțiile instant în consolă\n",
    "\n",
    "Acest tip de arhitectură poate fi extins cu Kafka, socket-uri TCP sau API-uri REST pentru cazuri industriale reale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4d626",
   "metadata": {},
   "source": [
    "Ce poți face pentru demo local?\n",
    "Creează un folder stream_input.\n",
    "\n",
    "Adaugă câte un fișier .csv pe rând cu date reale (ex: row_1.csv, row_2.csv, etc.).\n",
    "\n",
    "Spark va citi automat fiecare nou fișier și aplica modelul.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
