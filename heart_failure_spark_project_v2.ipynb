{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe6e0aa",
   "metadata": {},
   "source": [
    "# 1. Introducere\n",
    "\n",
    "## Prezentarea succintă a setului de date\n",
    "\n",
    "Setul de date utilizat este **Heart Failure Prediction**, disponibil pe Kaggle:\n",
    "https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction\n",
    "\n",
    "Setul de date conține 918 înregistrări despre pacienți, fiecare cu 12 caracteristici medicale și o variabilă țintă `HeartDisease` care indică dacă pacientul **are sau nu boală cardiovasculară**.\n",
    "\n",
    "## Obiective\n",
    "\n",
    "Obiectivul proiectului este:\n",
    "- **procesarea**, **curățarea** și **analizarea** setului de date folosind *Spark*;\n",
    "- **antrenarea** și **evaluarea** a două modele *ML*;\n",
    "- **aplicarea** unei metode de *DL* cu *TensorFlow*;\n",
    "- **implementarea** unui *pipeline* și *UDF*;\n",
    "- **integrarea** unui *flux de date în timp real* cu *Spark Streaming*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19630cf",
   "metadata": {},
   "source": [
    "# 2. Procesarea datelor cu Spark\n",
    "\n",
    "Vom folosi *PySpark* pentru a **analiza** și **prelucra** datele. Se vor aplica **agregări** și **transformări** folosind atât *DataFrame API* cât și *Spark SQL*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a2ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/15 08:17:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- ChestPainType: string (nullable = true)\n",
      " |-- RestingBP: integer (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- FastingBS: integer (nullable = true)\n",
      " |-- RestingECG: string (nullable = true)\n",
      " |-- MaxHR: integer (nullable = true)\n",
      " |-- ExerciseAngina: string (nullable = true)\n",
      " |-- Oldpeak: double (nullable = true)\n",
      " |-- ST_Slope: string (nullable = true)\n",
      " |-- HeartDisease: integer (nullable = true)\n",
      "\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "| 40|  M|          ATA|      140|        289|        0|    Normal|  172|             N|    0.0|      Up|           0|\n",
      "| 49|  F|          NAP|      160|        180|        0|    Normal|  156|             N|    1.0|    Flat|           1|\n",
      "| 37|  M|          ATA|      130|        283|        0|        ST|   98|             N|    0.0|      Up|           0|\n",
      "| 48|  F|          ASY|      138|        214|        0|    Normal|  108|             Y|    1.5|    Flat|           1|\n",
      "| 54|  M|          NAP|      150|        195|        0|    Normal|  122|             N|    0.0|      Up|           0|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HeartFailurePrediction\").getOrCreate()\n",
    "\n",
    "# Citirea setului de date\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"heart.csv\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205b5459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/15 08:17:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "|summary|               Age| Sex|ChestPainType|         RestingBP|       Cholesterol|          FastingBS|RestingECG|             MaxHR|ExerciseAngina|           Oldpeak|ST_Slope|       HeartDisease|\n",
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "|  count|               918| 918|          918|               918|               918|                918|       918|               918|           918|               918|     918|                918|\n",
      "|   mean|53.510893246187365|NULL|         NULL|132.39651416122004| 198.7995642701525|0.23311546840958605|      NULL|136.80936819172112|          NULL|0.8873638344226581|    NULL| 0.5533769063180828|\n",
      "| stddev|  9.43261650673202|NULL|         NULL|18.514154119907808|109.38414455220345|0.42304562473930296|      NULL| 25.46033413825029|          NULL|1.0665701510493264|    NULL|0.49741373828459706|\n",
      "|    min|                28|   F|          ASY|                 0|                 0|                  0|       LVH|                60|             N|              -2.6|    Down|                  0|\n",
      "|    max|                77|   M|           TA|               200|               603|                  1|        ST|               202|             Y|               6.2|      Up|                  1|\n",
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "\n",
      "+------------+-----+\n",
      "|HeartDisease|count|\n",
      "+------------+-----+\n",
      "|           1|  508|\n",
      "|           0|  410|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistici descriptive\n",
    "df.describe().show()\n",
    "\n",
    "# Distribuția țintei (HeartDisease)\n",
    "df.groupBy(\"HeartDisease\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec757d",
   "metadata": {},
   "source": [
    "## Curățare și transformare date\n",
    "\n",
    "**Verificăm** *valori lipsă* și **realizăm transformări** simple: *conversia la lowercase*, *eliminarea duplicatelor*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfacaca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|  0|  0|            0|        0|          0|        0|         0|    0|             0|      0|       0|           0|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificare valori lipsă\n",
    "from pyspark.sql.functions import col, isnan, when, count, trim\n",
    "\n",
    "# Pentru fiecare coloana din set-ul nostru de date, numaram valorile egale cu NULL sau stringurile vide.\n",
    "df.select([\n",
    "    count(\n",
    "        when(\n",
    "            col(c).isNull() | (trim(col(c)) == \"\"), c\n",
    "        )\n",
    "    ).alias(c)\n",
    "    for c in df.columns\n",
    "]).show()\n",
    "\n",
    "# Eliminare duplicate (dacă există)\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c0cf9",
   "metadata": {},
   "source": [
    "Din fericire set-ul de date pe care l-am ales **nu a continut** *NULL* sau *stringuri vide*, dar acest lucru **nu este valabil** pentru orice dataset, deci acesta prelucrare **este obligatorie**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39a424",
   "metadata": {},
   "source": [
    "## Grupări și agregări cu DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e72a9",
   "metadata": {},
   "source": [
    "Agregare: procentul bolnavilor pe gen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0df7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|Sex|HeartDiseaseRatePercent|\n",
      "+---+-----------------------+\n",
      "|  F|                  25.91|\n",
      "|  M|                  63.17|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round, avg\n",
    "\n",
    "df.groupBy(\"Sex\") \\\n",
    "  .agg(round(avg(\"HeartDisease\") * 100, 2).alias(\"HeartDiseaseRatePercent\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d874e34",
   "metadata": {},
   "source": [
    "Grupare pe categorii de vârstă:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8fd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|AgeGroup|HeartDiseaseRatePercent|\n",
      "+--------+-----------------------+\n",
      "|   40-59|                  50.77|\n",
      "|     60+|                  73.12|\n",
      "|     <40|                   32.5|\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn(\"AgeGroup\", when(col(\"Age\") < 40, \"<40\")\n",
    "                              .when((col(\"Age\") >= 40) & (col(\"Age\") < 60), \"40-59\")\n",
    "                              .otherwise(\"60+\"))\n",
    "\n",
    "df.groupBy(\"AgeGroup\") \\\n",
    "  .agg(round(avg(\"HeartDisease\") * 100, 2).alias(\"HeartDiseaseRatePercent\")) \\\n",
    "  .orderBy(\"AgeGroup\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2192ffd",
   "metadata": {},
   "source": [
    "## Interogări cu Spark SQL\n",
    "\n",
    "Rata bolii în funcție de *glicemie* (**FastingBS**) și *tensiune arterială* (**RestingBP**):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e79c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----------------------+\n",
      "|Diabetic|HighBP|Total|HeartDiseaseRatePercent|\n",
      "+--------+------+-----+-----------------------+\n",
      "|       1|     0|   80|                   80.0|\n",
      "|       1|     1|  134|                   79.1|\n",
      "|       0|     1|  409|                  50.86|\n",
      "|       0|     0|  295|                  44.07|\n",
      "+--------+------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"HighBP\", when(col(\"RestingBP\") >= 130, 1).otherwise(0))\n",
    "df.createOrReplaceTempView(\"heart\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "       FastingBS AS Diabetic,\n",
    "       HighBP,\n",
    "       COUNT(*) AS Total,\n",
    "       ROUND(AVG(HeartDisease) * 100, 2) AS HeartDiseaseRatePercent\n",
    "    FROM heart\n",
    "    GROUP BY FastingBS, HighBP\n",
    "    ORDER BY HeartDiseaseRatePercent DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62bf99",
   "metadata": {},
   "source": [
    "# 3. Metode de Machine Learning\n",
    "\n",
    "Vom **construi** și **evalua** *două modele* de *clasificare binară* pentru a **prezice** apariția *bolii cardiace (`HeartDisease`)*:\n",
    "\n",
    "1. *Logistic Regression*\n",
    "2. *Random Forest Classifier*\n",
    "\n",
    "Scopul este să comparăm performanța celor două metode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4417276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+-----+\n",
      "|features                                            |label|\n",
      "+----------------------------------------------------+-----+\n",
      "|[53.0,124.0,260.0,0.0,112.0,3.0,0.0,0.0,2.0,1.0,0.0]|1.0  |\n",
      "|[60.0,102.0,318.0,0.0,160.0,0.0,1.0,1.0,0.0,0.0,1.0]|1.0  |\n",
      "|[47.0,138.0,257.0,0.0,156.0,0.0,0.0,1.0,1.0,0.0,1.0]|1.0  |\n",
      "|(11,[0,1,2,4,7,10],[40.0,140.0,289.0,172.0,2.0,1.0])|1.0  |\n",
      "|(11,[0,1,2,4,7,10],[36.0,130.0,209.0,178.0,1.0,1.0])|1.0  |\n",
      "+----------------------------------------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Selectăm coloanele numerice\n",
    "feature_cols = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "# Transformăm și coloanele categorice în numeric (dacă există)\n",
    "categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\") for col in categorical_cols]\n",
    "\n",
    "# Asamblare vector caracteristici\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols + [col + \"_idx\" for col in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Etichetă (HeartDisease este deja 0/1 dar asigurăm consistența)\n",
    "label_indexer = StringIndexer(inputCol=\"HeartDisease\", outputCol=\"label\")\n",
    "\n",
    "# Pipeline de preprocesare\n",
    "pipeline = Pipeline(stages=indexers + [assembler, label_indexer])\n",
    "data = pipeline.fit(df).transform(df).select(\"features\", \"label\")\n",
    "data.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd804f8",
   "metadata": {},
   "source": [
    "## Împărțirea datelor\n",
    "\n",
    "Vom împărți setul de date în două subseturi: antrenare (70%) și test (30%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79a93d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 681, Test: 237\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "print(f\"Train: {train_data.count()}, Test: {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a80236",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "- Vrem să **prezicem** dacă o persoană are *boală cardiacă* (*HeartDisease = 1*) pe baza unor *caracteristici clinice*.\n",
    "\n",
    "- **Logistic Regression** este o alegere bună pentru *clasificare binară*, deoarece este *interpretabil*, *rapid de antrenat* și oferă o primă linie de bază.\n",
    "\n",
    "- **Aplicăm** *modelul* și **evaluăm** *acuratețea* pe *setul de testare*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (Logistic Regression): 0.8969\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "lr_auc = evaluator.evaluate(lr_predictions)\n",
    "print(f\"AUC (Logistic Regression): {lr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9517a5",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "La fel ca mai sus, dorim să facem clasificare binară.\n",
    "\n",
    "Random Forest este un model de tip \"ensemble\" ce poate surprinde relații non-liniare între variabile. Este mai robust decât Logistic Regression, dar mai lent.\n",
    "\n",
    "Antrenăm un model RF și comparăm performanța sa cu Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f43b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (Random Forest): 0.9155\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, seed=42)\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "rf_auc = evaluator.evaluate(rf_predictions)\n",
    "print(f\"AUC (Random Forest): {rf_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44521f75",
   "metadata": {},
   "source": [
    "## Concluzii ML\n",
    "\n",
    "Pentru ambele modele am folosit *AUC* - (**Area Under Curve**). \n",
    "\n",
    "Aceasta este o *metrică standard* folosită pentru *evaluarea performanței* unui model de *clasificare binară*, în special când clasele sunt *dezechilibrate*.\n",
    "\n",
    "| Model               | AUC         |\n",
    "|--------------------|-------------|\n",
    "| *Logistic Regression*| ~ *0.8969* |\n",
    "| *Random Forest*      | ~ *0.9155* |\n",
    "\n",
    "*Random Forest* a oferit performanță mai bună datorită capacității sale de a **modela relații complexe** între trăsături. Totuși, *Logistic Regression* este **util** ca *model de bază* și pentru *interpretabilitate*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519008d",
   "metadata": {},
   "source": [
    "## 4. Utilizarea unui Data Pipeline complet\n",
    "\n",
    "Vom **construi** un *Pipeline* care include:\n",
    "\n",
    "- **Preprocesare**: *indexare categorice* + *VectorAssembler*\n",
    "- **Model ML**: *Logistic Regression*\n",
    "\n",
    "Ne dorim să automatizăm întreaga secvență de transformări și învățare automată.\n",
    "\n",
    "Etape: \n",
    "\n",
    "1. **Indexare categorică**: Coloanele *string* sunt transformate în coloane numerice folosind *StringIndexer*.\n",
    "\n",
    "2. **VectorAssembler**: Toate coloanele numerice și cele indexate sunt combinate într-un singur vector de trăsături (*features*) necesar pentru modele ML.\n",
    "\n",
    "3. **Label Indexing**: Coloana țintă *HeartDisease* este transformată în eticheta *label*.\n",
    "\n",
    "4. **Modelul**: Se utilizează *LogisticRegression* pentru clasificare binară.\n",
    "\n",
    "5. **Pipeline**: Toți pașii anteriori sunt legați într-un *Pipeline* unitar ce permite aplicarea secvențială a etapelor.\n",
    "\n",
    "6. **Antrenare & Predicție**: *fit()* antrenează pipeline-ul pe date, *transform()* aplică modelul pe același set.\n",
    "\n",
    "7. **Evaluare**: Se utilizează *BinaryClassificationEvaluator* cu metrica *areaUnderROC* pentru a evalua performanța modelului."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b8b8156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC cu pipeline complet (Logistic Regression): 0.9194\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Refacem pipeline-ul complet: indexeri + assembler + model\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\") for col in categorical_cols]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols + [col + \"_idx\" for col in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "label_indexer = StringIndexer(inputCol=\"HeartDisease\", outputCol=\"label\")\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Pipeline complet\n",
    "full_pipeline = Pipeline(stages=indexers + [assembler, label_indexer, lr])\n",
    "\n",
    "# Antrenare + predicție\n",
    "pipeline_model = full_pipeline.fit(df)\n",
    "predictions = pipeline_model.transform(df)\n",
    "\n",
    "# Evaluare rapidă\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "pipeline_auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC cu pipeline complet (Logistic Regression): {pipeline_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e917c",
   "metadata": {},
   "source": [
    "## 5. Utilizarea unei funcții definite de utilizator (UDF)\n",
    "\n",
    "Vom crea o funcție UDF care calculează un scor de risc personalizat în funcție de vârstă și colesterol.\n",
    "\n",
    "Apoi, vom adăuga acest scor ca o coloană nouă pentru analiză sau input în model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677270ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Funcție de calcul risc: rudimentară pentru demonstrație\n",
    "def risk_score(age, cholesterol):\n",
    "    score = 0.5 * age + 0.5 * (cholesterol if cholesterol else 0)\n",
    "    return float(score / 200)  # normalizare\n",
    "\n",
    "risk_udf = udf(risk_score, DoubleType())\n",
    "\n",
    "# Aplicare UDF\n",
    "df_with_risk = df.withColumn(\"RiskScore\", risk_udf(col(\"Age\"), col(\"Cholesterol\")))\n",
    "df_with_risk.select(\"Age\", \"Cholesterol\", \"RiskScore\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127762d8",
   "metadata": {},
   "source": [
    "## Optimizarea hiperparametrilor (Grid Search)\n",
    "\n",
    "Vom aplica o căutare în grilă (`ParamGridBuilder`) și validare încrucișată (`CrossValidator`) pentru Logistic Regression.\n",
    "\n",
    "Scopul: găsirea valorii optime pentru `regParam` și `elasticNetParam`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7802870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Pipeline de bază (doar cu Logistic Regression)\n",
    "pipeline_lr = Pipeline(stages=indexers + [assembler, label_indexer, lr])\n",
    "\n",
    "# Grid de parametri\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Execută căutarea\n",
    "cv_model = crossval.fit(df)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Afișăm AUC pe modelul optimizat\n",
    "cv_auc = evaluator.evaluate(best_model.transform(df))\n",
    "print(f\"AUC cu Logistic Regression + param tuning: {cv_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda3159",
   "metadata": {},
   "source": [
    "## Concluzie finală\n",
    "\n",
    "Am integrat într-un pipeline complet pașii de preprocesare, antrenare și evaluare. Am utilizat:\n",
    "\n",
    "- UDF pentru definirea unei logici personalizate de scor de risc\n",
    "- Grid search și validare încrucișată pentru optimizarea hiperparametrilor\n",
    "\n",
    "Astfel, proiectul acoperă toate etapele unui flux de ML profesionist cu Spark MLlib.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ad205",
   "metadata": {},
   "source": [
    "# 6. Deep Learning cu TensorFlow\n",
    "\n",
    "## Problemă\n",
    "\n",
    "Dorim să construim un model de clasificare binară care prezice apariția unei boli cardiace (`HeartDisease = 0 sau 1`) folosind rețele neuronale.\n",
    "\n",
    "## Justificare\n",
    "\n",
    "Rețelele neuronale artificiale pot capta relații complexe, neliniare între variabile, mai ales când există mai multe atribute implicate. În comparație cu Logistic Regression sau Random Forest, DL oferă flexibilitate mai mare pentru modelarea relațiilor nelineare.\n",
    "\n",
    "## Soluție\n",
    "\n",
    "Vom folosi TensorFlow + Keras pentru a antrena o rețea neuronală simplă cu:\n",
    "\n",
    "- 2 straturi ascunse (dense)\n",
    "- Funcția de activare `ReLU`\n",
    "- Funcția de pierdere `binary_crossentropy`\n",
    "- Optimizator `Adam`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Salvăm datele Spark într-un Pandas DataFrame pentru TensorFlow\n",
    "pandas_df = df.select(\n",
    "    \"Age\", \"Sex\", \"ChestPainType\", \"RestingBP\", \"Cholesterol\",\n",
    "    \"FastingBS\", \"RestingECG\", \"MaxHR\", \"ExerciseAngina\", \n",
    "    \"Oldpeak\", \"ST_Slope\", \"HeartDisease\"\n",
    ").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72493f",
   "metadata": {},
   "source": [
    "## Preprocesare date pentru TensorFlow\n",
    "\n",
    "Vom converti datele categorice în one-hot encoding și vom scala datele numerice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2804f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Separăm features și label\n",
    "X = pandas_df.drop(\"HeartDisease\", axis=1)\n",
    "y = pandas_df[\"HeartDisease\"]\n",
    "\n",
    "# Coloane categorice și numerice\n",
    "cat_cols = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"]\n",
    "num_cols = [\"Age\", \"RestingBP\", \"Cholesterol\", \"FastingBS\", \"MaxHR\", \"Oldpeak\"]\n",
    "\n",
    "# Pipeline de transformare\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(), cat_cols)\n",
    "])\n",
    "\n",
    "# Aplicăm transformările\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Împărțire în train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a424e0",
   "metadata": {},
   "source": [
    "## Construirea modelului TensorFlow\n",
    "\n",
    "Model secvențial cu 2 straturi ascunse și un strat final cu sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d383f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # output pentru clasificare binară\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7104ec",
   "metadata": {},
   "source": [
    "## Antrenarea modelului\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77382587",
   "metadata": {},
   "source": [
    "## Evaluarea modelului pe setul de testare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23991ca7",
   "metadata": {},
   "source": [
    "## Concluzie Deep Learning\n",
    "\n",
    "Modelul de rețea neuronală oferă o acuratețe competitivă pe setul de testare. \n",
    "\n",
    "Avantaje:\n",
    "- Capacitate de modelare complexă\n",
    "- Poate învăța relații subtile în date\n",
    "\n",
    "Dezavantaje:\n",
    "- Necesită mai multă putere computațională\n",
    "- Mai puțin interpretabil decât modele simple\n",
    "\n",
    "Modelul poate fi îmbunătățit prin regularizare, dropout sau ajustarea hiperparametrilor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad6ae8",
   "metadata": {},
   "source": [
    "# 7. Streaming în Spark cu inferență ML în timp real\n",
    "\n",
    "## Problemă\n",
    "\n",
    "Simulăm un flux de date (stream) de la un fișier CSV care este completat treptat. Vom folosi `Spark Structured Streaming` pentru a procesa în timp real și a aplica **modelul ML Logistic Regression antrenat anterior** pentru a face inferență pe fiecare lot de date nou sosite.\n",
    "\n",
    "## Justificare\n",
    "\n",
    "În aplicații reale precum monitorizarea pacienților, datele vin în timp real. Spark Structured Streaming ne permite să facem inferență în timp real la scară mare.\n",
    "\n",
    "## Soluție\n",
    "\n",
    "1. Folosim un director monitorizat ca sursă de date (CSV-uri simulate incremental).\n",
    "2. Aplicăm același pipeline de preprocesare și predicție deja antrenat.\n",
    "3. Afișăm rezultatele în timp real în consolă.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c88d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Definim schema explicită (pentru streaming)\n",
    "schema = StructType() \\\n",
    "    .add(\"Age\", IntegerType()) \\\n",
    "    .add(\"Sex\", StringType()) \\\n",
    "    .add(\"ChestPainType\", StringType()) \\\n",
    "    .add(\"RestingBP\", IntegerType()) \\\n",
    "    .add(\"Cholesterol\", IntegerType()) \\\n",
    "    .add(\"FastingBS\", IntegerType()) \\\n",
    "    .add(\"RestingECG\", StringType()) \\\n",
    "    .add(\"MaxHR\", IntegerType()) \\\n",
    "    .add(\"ExerciseAngina\", StringType()) \\\n",
    "    .add(\"Oldpeak\", DoubleType()) \\\n",
    "    .add(\"ST_Slope\", StringType()) \\\n",
    "    .add(\"HeartDisease\", IntegerType())  # eticheta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a034a",
   "metadata": {},
   "source": [
    "## Inițializare flux streaming dintr-un director\n",
    "\n",
    "Simulăm fluxul adăugând incremental fișiere CSV în directorul `stream_input/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaec338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citim streaming dintr-un director local (CSV)\n",
    "stream_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .csv(\"stream_input/\")  # creezi acest director și pui fișierele treptat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411a349",
   "metadata": {},
   "source": [
    "## Aplicarea modelului Logistic Regression antrenat anterior pe fiecare micro-batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folosim pipeline-ul deja antrenat (de la punctul anterior)\n",
    "# pipeline_model este un model de tip PipelineModel cu LogisticRegression\n",
    "\n",
    "# Aplicăm transformări și predicții pe fluxul de date\n",
    "predictions_stream = pipeline_model.transform(stream_df)\n",
    "\n",
    "# Selectăm doar coloanele relevante\n",
    "results = predictions_stream.select(\"Age\", \"Sex\", \"Cholesterol\", \"prediction\", \"probability\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140c42b",
   "metadata": {},
   "source": [
    "## Scriem rezultatele în consolă în mod continuu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdbb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = results.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7446df",
   "metadata": {},
   "source": [
    "## Concluzie\n",
    "\n",
    "Am implementat un flux de streaming Spark care:\n",
    "\n",
    "- Monitorizează fișiere CSV în timp real\n",
    "- Aplică un model ML (Logistic Regression) pe fiecare batch de date\n",
    "- Returnează predicțiile instant în consolă\n",
    "\n",
    "Acest tip de arhitectură poate fi extins cu Kafka, socket-uri TCP sau API-uri REST pentru cazuri industriale reale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4d626",
   "metadata": {},
   "source": [
    "Ce poți face pentru demo local?\n",
    "Creează un folder stream_input.\n",
    "\n",
    "Adaugă câte un fișier .csv pe rând cu date reale (ex: row_1.csv, row_2.csv, etc.).\n",
    "\n",
    "Spark va citi automat fiecare nou fișier și aplica modelul.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
