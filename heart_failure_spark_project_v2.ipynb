{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94301caf",
   "metadata": {},
   "source": [
    "# ğŸ©º PredicÈ›ia bolilor de inimÄƒ folosind PySpark, MLlib È™i TensorFlow\n",
    "\n",
    "## **Mihai Andrei-Alexandru**, *Big Data Project*, *iunie 2025*\n",
    "\n",
    "---\n",
    "---\n",
    "## Cuprins\n",
    "\n",
    "1. [Introducere](#1-introducere)\n",
    "2. [Procesarea datelor cu Spark](#2-procesarea-datelor-cu-spark)\n",
    "3. [Metode de Machine Learning](#3-metode-de-machine-learning)\n",
    "4. [Data Pipeline complet](#4-utilizarea-unui-data-pipeline-complet)\n",
    "5. [Functii definite de utilizator (UDF)](#5-utilizarea-unei-funcÈ›ii-definite-de-utilizator-udf)\n",
    "6. [Deep Learning cu TensorFlow](#5-utilizarea-unei-funcÈ›ii-definite-de-utilizator-udf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6e0aa",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 1. Introducere\n",
    "\n",
    "## Prezentarea succintÄƒ a setului de date\n",
    "\n",
    "Setul de date utilizat este **Heart Failure Prediction**, disponibil pe Kaggle:\n",
    "https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction\n",
    "\n",
    "Setul de date conÈ›ine 918 Ã®nregistrÄƒri despre pacienÈ›i, fiecare cu 12 caracteristici medicale È™i o variabilÄƒ È›intÄƒ `HeartDisease` care indicÄƒ dacÄƒ pacientul **are sau nu boalÄƒ cardiovascularÄƒ**.\n",
    "\n",
    "## Obiective\n",
    "\n",
    "Obiectivul proiectului este:\n",
    "- **procesarea**, **curÄƒÈ›area** È™i **analizarea** setului de date folosind *Spark*;\n",
    "- **antrenarea** È™i **evaluarea** a douÄƒ modele *ML*;\n",
    "- **aplicarea** unei metode de *DL* cu *TensorFlow*;\n",
    "- **implementarea** unui *pipeline* È™i *UDF*;\n",
    "- **integrarea** unui *flux de date Ã®n timp real* cu *Spark Streaming*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19630cf",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 2. Procesarea datelor cu Spark\n",
    "\n",
    "Vom folosi *PySpark* pentru a **analiza** È™i **prelucra** datele. Se vor aplica **agregÄƒri** È™i **transformÄƒri** folosind atÃ¢t *DataFrame API* cÃ¢t È™i *Spark SQL*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7a2ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/15 09:56:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- ChestPainType: string (nullable = true)\n",
      " |-- RestingBP: integer (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- FastingBS: integer (nullable = true)\n",
      " |-- RestingECG: string (nullable = true)\n",
      " |-- MaxHR: integer (nullable = true)\n",
      " |-- ExerciseAngina: string (nullable = true)\n",
      " |-- Oldpeak: double (nullable = true)\n",
      " |-- ST_Slope: string (nullable = true)\n",
      " |-- HeartDisease: integer (nullable = true)\n",
      "\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "| 40|  M|          ATA|      140|        289|        0|    Normal|  172|             N|    0.0|      Up|           0|\n",
      "| 49|  F|          NAP|      160|        180|        0|    Normal|  156|             N|    1.0|    Flat|           1|\n",
      "| 37|  M|          ATA|      130|        283|        0|        ST|   98|             N|    0.0|      Up|           0|\n",
      "| 48|  F|          ASY|      138|        214|        0|    Normal|  108|             Y|    1.5|    Flat|           1|\n",
      "| 54|  M|          NAP|      150|        195|        0|    Normal|  122|             N|    0.0|      Up|           0|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HeartFailurePrediction\").getOrCreate()\n",
    "\n",
    "# Citirea setului de date\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"heart.csv\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "205b5459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/15 09:56:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "|summary|               Age| Sex|ChestPainType|         RestingBP|       Cholesterol|          FastingBS|RestingECG|             MaxHR|ExerciseAngina|           Oldpeak|ST_Slope|       HeartDisease|\n",
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "|  count|               918| 918|          918|               918|               918|                918|       918|               918|           918|               918|     918|                918|\n",
      "|   mean|53.510893246187365|NULL|         NULL|132.39651416122004| 198.7995642701525|0.23311546840958605|      NULL|136.80936819172112|          NULL|0.8873638344226581|    NULL| 0.5533769063180828|\n",
      "| stddev|  9.43261650673202|NULL|         NULL|18.514154119907808|109.38414455220345|0.42304562473930296|      NULL| 25.46033413825029|          NULL|1.0665701510493264|    NULL|0.49741373828459706|\n",
      "|    min|                28|   F|          ASY|                 0|                 0|                  0|       LVH|                60|             N|              -2.6|    Down|                  0|\n",
      "|    max|                77|   M|           TA|               200|               603|                  1|        ST|               202|             Y|               6.2|      Up|                  1|\n",
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "\n",
      "+------------+-----+\n",
      "|HeartDisease|count|\n",
      "+------------+-----+\n",
      "|           1|  508|\n",
      "|           0|  410|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistici descriptive\n",
    "df.describe().show()\n",
    "\n",
    "# DistribuÈ›ia È›intei (HeartDisease)\n",
    "df.groupBy(\"HeartDisease\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec757d",
   "metadata": {},
   "source": [
    "## CurÄƒÈ›are È™i transformare date\n",
    "\n",
    "**VerificÄƒm** *valori lipsÄƒ* È™i **realizÄƒm transformÄƒri** simple: *conversia la lowercase*, *eliminarea duplicatelor*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfacaca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|  0|  0|            0|        0|          0|        0|         0|    0|             0|      0|       0|           0|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificare valori lipsÄƒ\n",
    "from pyspark.sql.functions import col, isnan, when, count, trim\n",
    "\n",
    "# Pentru fiecare coloana din set-ul nostru de date, numaram valorile egale cu NULL sau stringurile vide.\n",
    "df.select([\n",
    "    count(\n",
    "        when(\n",
    "            col(c).isNull() | (trim(col(c)) == \"\"), c\n",
    "        )\n",
    "    ).alias(c)\n",
    "    for c in df.columns\n",
    "]).show()\n",
    "\n",
    "# Eliminare duplicate (dacÄƒ existÄƒ)\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c0cf9",
   "metadata": {},
   "source": [
    "Din fericire set-ul de date pe care l-am ales **nu a continut** *NULL* sau *stringuri vide*, dar acest lucru **nu este valabil** pentru orice dataset, deci acesta prelucrare **este obligatorie**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39a424",
   "metadata": {},
   "source": [
    "## GrupÄƒri È™i agregÄƒri cu DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e72a9",
   "metadata": {},
   "source": [
    "Agregare: procentul bolnavilor pe gen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0df7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|Sex|HeartDiseaseRatePercent|\n",
      "+---+-----------------------+\n",
      "|  F|                  25.91|\n",
      "|  M|                  63.17|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round, avg\n",
    "\n",
    "df.groupBy(\"Sex\") \\\n",
    "  .agg(round(avg(\"HeartDisease\") * 100, 2).alias(\"HeartDiseaseRatePercent\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d874e34",
   "metadata": {},
   "source": [
    "Grupare pe categorii de vÃ¢rstÄƒ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8fd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|AgeGroup|HeartDiseaseRatePercent|\n",
      "+--------+-----------------------+\n",
      "|   40-59|                  50.77|\n",
      "|     60+|                  73.12|\n",
      "|     <40|                   32.5|\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn(\"AgeGroup\", when(col(\"Age\") < 40, \"<40\")\n",
    "                              .when((col(\"Age\") >= 40) & (col(\"Age\") < 60), \"40-59\")\n",
    "                              .otherwise(\"60+\"))\n",
    "\n",
    "df.groupBy(\"AgeGroup\") \\\n",
    "  .agg(round(avg(\"HeartDisease\") * 100, 2).alias(\"HeartDiseaseRatePercent\")) \\\n",
    "  .orderBy(\"AgeGroup\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2192ffd",
   "metadata": {},
   "source": [
    "## InterogÄƒri cu Spark SQL\n",
    "\n",
    "Rata bolii Ã®n funcÈ›ie de *glicemie* (**FastingBS**) È™i *tensiune arterialÄƒ* (**RestingBP**):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e79c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----------------------+\n",
      "|Diabetic|HighBP|Total|HeartDiseaseRatePercent|\n",
      "+--------+------+-----+-----------------------+\n",
      "|       1|     0|   80|                   80.0|\n",
      "|       1|     1|  134|                   79.1|\n",
      "|       0|     1|  409|                  50.86|\n",
      "|       0|     0|  295|                  44.07|\n",
      "+--------+------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"HighBP\", when(col(\"RestingBP\") >= 130, 1).otherwise(0))\n",
    "df.createOrReplaceTempView(\"heart\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "       FastingBS AS Diabetic,\n",
    "       HighBP,\n",
    "       COUNT(*) AS Total,\n",
    "       ROUND(AVG(HeartDisease) * 100, 2) AS HeartDiseaseRatePercent\n",
    "    FROM heart\n",
    "    GROUP BY FastingBS, HighBP\n",
    "    ORDER BY HeartDiseaseRatePercent DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62bf99",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 3. Metode de Machine Learning\n",
    "\n",
    "Vom **construi** È™i **evalua** *douÄƒ modele* de *clasificare binarÄƒ* pentru a **prezice** apariÈ›ia *bolii cardiace (`HeartDisease`)*:\n",
    "\n",
    "1. *Logistic Regression*\n",
    "2. *Random Forest Classifier*\n",
    "\n",
    "Scopul este sÄƒ comparÄƒm performanÈ›a celor douÄƒ metode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4417276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+-----+\n",
      "|features                                            |label|\n",
      "+----------------------------------------------------+-----+\n",
      "|(11,[0,1,2,4,7,10],[40.0,140.0,289.0,172.0,2.0,1.0])|1.0  |\n",
      "|[49.0,160.0,180.0,0.0,156.0,1.0,1.0,1.0,0.0,0.0,0.0]|0.0  |\n",
      "|[37.0,130.0,283.0,0.0,98.0,0.0,0.0,2.0,2.0,0.0,1.0] |1.0  |\n",
      "|[48.0,138.0,214.0,0.0,108.0,1.5,1.0,0.0,0.0,1.0,0.0]|0.0  |\n",
      "|(11,[0,1,2,4,7,10],[54.0,150.0,195.0,122.0,1.0,1.0])|1.0  |\n",
      "+----------------------------------------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# SelectÄƒm coloanele numerice\n",
    "feature_cols = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "# TransformÄƒm È™i coloanele categorice Ã®n numeric (dacÄƒ existÄƒ)\n",
    "categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\") for col in categorical_cols]\n",
    "\n",
    "# Asamblare vector caracteristici\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols + [col + \"_idx\" for col in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# EtichetÄƒ (HeartDisease este deja 0/1 dar asigurÄƒm consistenÈ›a)\n",
    "label_indexer = StringIndexer(inputCol=\"HeartDisease\", outputCol=\"label\")\n",
    "\n",
    "# Pipeline de preprocesare\n",
    "pipeline = Pipeline(stages=indexers + [assembler, label_indexer])\n",
    "data = pipeline.fit(df).transform(df).select(\"features\", \"label\")\n",
    "data.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd804f8",
   "metadata": {},
   "source": [
    "## ÃmpÄƒrÈ›irea datelor\n",
    "\n",
    "Vom Ã®mpÄƒrÈ›i setul de date Ã®n douÄƒ subseturi: antrenare (70%) È™i test (30%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a93d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 681, Test: 237\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "print(f\"Train: {train_data.count()}, Test: {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a80236",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Vrem sÄƒ **prezicem** dacÄƒ o persoanÄƒ are *boalÄƒ cardiacÄƒ* (*HeartDisease = 1*) pe baza unor *caracteristici clinice*.\n",
    "\n",
    "- **Logistic Regression** este o alegere bunÄƒ pentru *clasificare binarÄƒ*, deoarece este *interpretabil*, *rapid de antrenat* È™i oferÄƒ o primÄƒ linie de bazÄƒ.\n",
    "\n",
    "- **AplicÄƒm** *modelul* È™i **evaluÄƒm** *acurateÈ›ea* pe *setul de testare*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (Logistic Regression): 0.8969\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "lr_auc = evaluator.evaluate(lr_predictions)\n",
    "print(f\"AUC (Logistic Regression): {lr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9517a5",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "La fel ca mai sus, dorim sÄƒ facem clasificare binarÄƒ.\n",
    "\n",
    "Random Forest este un model de tip \"ensemble\" ce poate surprinde relaÈ›ii non-liniare Ã®ntre variabile. Este mai robust decÃ¢t Logistic Regression, dar mai lent.\n",
    "\n",
    "AntrenÄƒm un model RF È™i comparÄƒm performanÈ›a sa cu Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f43b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (Random Forest): 0.9155\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, seed=42)\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "rf_auc = evaluator.evaluate(rf_predictions)\n",
    "print(f\"AUC (Random Forest): {rf_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44521f75",
   "metadata": {},
   "source": [
    "## Concluzii ML\n",
    "\n",
    "Pentru ambele modele am folosit *AUC* - (**Area Under Curve**). \n",
    "\n",
    "Aceasta este o *metricÄƒ standard* folositÄƒ pentru *evaluarea performanÈ›ei* unui model de *clasificare binarÄƒ*, Ã®n special cÃ¢nd clasele sunt *dezechilibrate*.\n",
    "\n",
    "| Model               | AUC         |\n",
    "|--------------------|-------------|\n",
    "| *Logistic Regression*| ~ *0.8969* |\n",
    "| *Random Forest*      | ~ *0.9155* |\n",
    "\n",
    "*Random Forest* a oferit performanÈ›Äƒ mai bunÄƒ datoritÄƒ capacitÄƒÈ›ii sale de a **modela relaÈ›ii complexe** Ã®ntre trÄƒsÄƒturi. TotuÈ™i, *Logistic Regression* este **util** ca *model de bazÄƒ* È™i pentru *interpretabilitate*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519008d",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 4. Utilizarea unui Data Pipeline complet\n",
    "\n",
    "Vom **construi** un *Pipeline* care include:\n",
    "\n",
    "- **Preprocesare**: *indexare categorice* + *VectorAssembler*\n",
    "- **Model ML**: *Logistic Regression*\n",
    "\n",
    "Ne dorim sÄƒ automatizÄƒm Ã®ntreaga secvenÈ›Äƒ de transformÄƒri È™i Ã®nvÄƒÈ›are automatÄƒ.\n",
    "\n",
    "Etape: \n",
    "\n",
    "1. **Indexare categoricÄƒ**: Coloanele *string* sunt transformate Ã®n coloane numerice folosind *StringIndexer*.\n",
    "\n",
    "2. **VectorAssembler**: Toate coloanele numerice È™i cele indexate sunt combinate Ã®ntr-un singur vector de trÄƒsÄƒturi (*features*) necesar pentru modele ML.\n",
    "\n",
    "3. **Label Indexing**: Coloana È›intÄƒ *HeartDisease* este transformatÄƒ Ã®n eticheta *label*.\n",
    "\n",
    "4. **Modelul**: Se utilizeazÄƒ *LogisticRegression* pentru clasificare binarÄƒ.\n",
    "\n",
    "5. **Pipeline**: ToÈ›i paÈ™ii anteriori sunt legaÈ›i Ã®ntr-un *Pipeline* unitar ce permite aplicarea secvenÈ›ialÄƒ a etapelor.\n",
    "\n",
    "6. **Antrenare & PredicÈ›ie**: *fit()* antreneazÄƒ pipeline-ul pe date, *transform()* aplicÄƒ modelul pe acelaÈ™i set.\n",
    "\n",
    "7. **Evaluare**: Se utilizeazÄƒ *BinaryClassificationEvaluator* cu metrica *areaUnderROC* pentru a evalua performanÈ›a modelului."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b8b8156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/15 09:57:34 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC cu pipeline complet (Logistic Regression): 0.9194\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Refacem pipeline-ul complet: indexeri + assembler + model\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\") for col in categorical_cols]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols + [col + \"_idx\" for col in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "label_indexer = StringIndexer(inputCol=\"HeartDisease\", outputCol=\"label\")\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Pipeline complet\n",
    "full_pipeline = Pipeline(stages=indexers + [assembler, label_indexer, lr])\n",
    "\n",
    "# Antrenare + predicÈ›ie\n",
    "pipeline_model = full_pipeline.fit(df)\n",
    "predictions = pipeline_model.transform(df)\n",
    "\n",
    "# Evaluare rapidÄƒ\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "pipeline_auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC cu pipeline complet (Logistic Regression): {pipeline_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e917c",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 5. Utilizarea unei funcÈ›ii definite de utilizator (UDF)\n",
    "\n",
    "Voi crea o funcÈ›ie care calculeazÄƒ un scor de risc personalizat Ã®n funcÈ›ie de vÃ¢rstÄƒ È™i colesterol, urmÃ¢nd sÄƒ adaug acest scor ca o coloanÄƒ nouÄƒ pentru analizÄƒ sau input Ã®n model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "677270ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+\n",
      "|Age|Cholesterol|RiskScore|\n",
      "+---+-----------+---------+\n",
      "| 40|        289|   0.8225|\n",
      "| 49|        180|   0.5725|\n",
      "| 37|        283|      0.8|\n",
      "| 48|        214|    0.655|\n",
      "| 54|        195|   0.6225|\n",
      "+---+-----------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# FuncÈ›ie de calcul risc: rudimentarÄƒ pentru demonstraÈ›ie\n",
    "def risk_score(age, cholesterol):\n",
    "    score = 0.5 * age + 0.5 * (cholesterol if cholesterol else 0)\n",
    "    return float(score / 200)  # normalizare\n",
    "\n",
    "risk_udf = udf(risk_score, DoubleType())\n",
    "\n",
    "# Aplicare UDF\n",
    "df_with_risk = df.withColumn(\"RiskScore\", risk_udf(col(\"Age\"), col(\"Cholesterol\")))\n",
    "df_with_risk.select(\"Age\", \"Cholesterol\", \"RiskScore\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127762d8",
   "metadata": {},
   "source": [
    "## Optimizarea hiperparametrilor (Grid Search)\n",
    "\n",
    "Voi **aplica** un *Grid Search* È™i *Cross Validation* pentru *Logistic Regression*.\n",
    "\n",
    "Ãmi doresc sÄƒ optimizez hiperparametrii, astfel, trebuie sÄƒ gÄƒsesc valoarea optimÄƒ pentru *regParam* È™i *elasticNetParam*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7802870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/15 10:13:50 WARN CacheManager: Asked to cache already cached data.\n",
      "25/06/15 10:13:50 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC cu Logistic Regression + param tuning: 0.9195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand']\n",
      "Bad pipe message: %s [b'ol: max-age=0\\r\\nsec-ch-ua: \"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"\\r\\nsec-ch-ua-mobile: ?0\\r\\n']\n",
      "Bad pipe message: %s [b'c-ch-ua-platform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) A', b'leWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\\r\\nAccept: text/html,application/xhtml+xml,app']\n",
      "Bad pipe message: %s [b'cation/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nSec-Fet', b'-Site: none\\r\\nSec-Fetch-Mode: navigate\\r\\nSec-Fetch-User: ?1\\r\\nSec-Fetch-Dest: document\\r\\nAccept-Encodi']\n",
      "Bad pipe message: %s [b'ol: max-age=0\\r\\nsec-ch-ua: \"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"\\r\\nsec-ch-ua-mobile: ?0\\r\\n']\n",
      "Bad pipe message: %s [b'c-ch-ua-platform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) A', b'leWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\\r\\nAccept: text/html,application/xhtml+xml,app']\n",
      "Bad pipe message: %s [b'cation/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nSec-Fet', b'-Site: cross-site\\r\\nSec-Fetch-Mode: navigate\\r\\nSec-Fetch-User: ?1\\r\\nSec-Fetch-Dest: document\\r\\nAccept-']\n",
      "Bad pipe message: %s [b'coding: gzip, deflate, br, zstd\\r\\nAccept-Language: en-US,en;q=0.9,ro;']\n",
      "Bad pipe message: %s [b'ol: max-age=0\\r\\nsec-ch-ua: \"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"\\r\\nsec-ch-ua-mobile: ?0\\r\\n']\n",
      "Bad pipe message: %s [b'c-ch-ua-platform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) A', b'leWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\\r\\nAccept: text/html,application/xhtml+xml,app']\n",
      "Bad pipe message: %s [b'cation/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nSec-Fet', b'-Site: cross-site\\r\\nSec-Fetch-Mode: navigate\\r\\nSec-Fetch-Dest: document\\r\\nAccept-Encoding: gzip, defl']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Pipeline de bazÄƒ (doar cu Logistic Regression)\n",
    "pipeline_lr = Pipeline(stages=indexers + [assembler, label_indexer, lr])\n",
    "\n",
    "# Grid de parametri\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# ExecutÄƒ cÄƒutarea\n",
    "cv_model = crossval.fit(df)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# AfiÈ™Äƒm AUC pe modelul optimizat\n",
    "cv_auc = evaluator.evaluate(best_model.transform(df))\n",
    "print(f\"AUC cu Logistic Regression + param tuning: {cv_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda3159",
   "metadata": {},
   "source": [
    "## Concluzii\n",
    "\n",
    "Am **integrat** Ã®ntr-un *pipeline complet* paÈ™ii de *preprocesare*, *antrenare* È™i *evaluare*. Am **utilizat**:\n",
    "\n",
    "- *UDF* pentru **definirea** unei *logici personalizate de scor de risc*\n",
    "- *Grid search* È™i *cross validation* pentru *optimizarea hiperparametrilor*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ad205",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 6. Deep Learning cu TensorFlow\n",
    "\n",
    "Dorim sÄƒ construim un model de clasificare binarÄƒ care prezice apariÈ›ia unei boli cardiace (*HeartDisease = 0 sau 1*) folosind reÈ›ele neuronale.\n",
    "\n",
    "ReÈ›elele neuronale artificiale pot capta relaÈ›ii complexe, neliniare Ã®ntre variabile, mai ales cÃ¢nd existÄƒ mai multe atribute implicate. Ãn comparaÈ›ie cu Logistic Regression sau Random Forest, DL oferÄƒ flexibilitate mai mare pentru modelarea relaÈ›iilor nelineare.\n",
    "\n",
    "Vom folosi TensorFlow + Keras pentru a antrena o reÈ›ea neuronalÄƒ simplÄƒ cu:\n",
    "\n",
    "- 2 straturi ascunse (dense)\n",
    "- FuncÈ›ia de activare *ReLU*\n",
    "- FuncÈ›ia de pierdere *binary_crossentropy*\n",
    "- Optimizator *Adam*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43d4abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# SalvÄƒm datele Spark Ã®ntr-un Pandas DataFrame pentru TensorFlow\n",
    "pandas_df = df.select(\n",
    "    \"Age\", \"Sex\", \"ChestPainType\", \"RestingBP\", \"Cholesterol\",\n",
    "    \"FastingBS\", \"RestingECG\", \"MaxHR\", \"ExerciseAngina\", \n",
    "    \"Oldpeak\", \"ST_Slope\", \"HeartDisease\"\n",
    ").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72493f",
   "metadata": {},
   "source": [
    "## Preprocesare date pentru TensorFlow\n",
    "\n",
    "Vom converti datele categorice Ã®n one-hot encoding È™i vom scala datele numerice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2804f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# SeparÄƒm features È™i label\n",
    "X = pandas_df.drop(\"HeartDisease\", axis=1)\n",
    "y = pandas_df[\"HeartDisease\"]\n",
    "\n",
    "# Coloane categorice È™i numerice\n",
    "cat_cols = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"]\n",
    "num_cols = [\"Age\", \"RestingBP\", \"Cholesterol\", \"FastingBS\", \"MaxHR\", \"Oldpeak\"]\n",
    "\n",
    "# Pipeline de transformare\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(), cat_cols)\n",
    "])\n",
    "\n",
    "# AplicÄƒm transformÄƒrile\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# ÃmpÄƒrÈ›ire Ã®n train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a424e0",
   "metadata": {},
   "source": [
    "## Construirea modelului TensorFlow\n",
    "\n",
    "Model secvenÈ›ial cu 2 straturi ascunse È™i un strat final cu sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d383f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 09:58:04.517672: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-15 09:58:04.518433: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-15 09:58:04.523461: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-15 09:58:04.532728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749981484.549408     952 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749981484.554505     952 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749981484.568485     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749981484.568501     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749981484.568502     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749981484.568503     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-15 09:58:04.572540: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-06-15 09:58:05.888882: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚           \u001b[38;5;34m672\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             â”‚           \u001b[38;5;34m528\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m17\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,217</span> (4.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,217\u001b[0m (4.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,217</span> (4.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,217\u001b[0m (4.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # output pentru clasificare binarÄƒ\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7104ec",
   "metadata": {},
   "source": [
    "## Antrenarea modelului\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c6f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77382587",
   "metadata": {},
   "source": [
    "## Evaluarea modelului pe setul de testare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8468 - loss: 0.3631 \n",
      "Test Accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23991ca7",
   "metadata": {},
   "source": [
    "## Concluzie Deep Learning\n",
    "\n",
    "Ãn aceastÄƒ secÈ›iune am folosit **TensorFlow** pentru a construi È™i antrena un model de reÈ›ea neuronalÄƒ care prezice probabilitatea de boalÄƒ cardiacÄƒ pe baza caracteristicilor pacienÈ›ilor. PaÈ™ii principali sunt:\n",
    "\n",
    "1. **Conversie Spark â†’ Pandas**:\n",
    "   - Datele Spark sunt convertite Ã®n format Pandas (`toPandas()`) pentru a fi compatibile cu TensorFlow È™i Scikit-learn.\n",
    "\n",
    "2. **Preprocesare date**:\n",
    "   - SeparÄƒm `X` (features) È™i `y` (eticheta).\n",
    "   - Coloanele numerice sunt scalate (`StandardScaler`) iar cele categorice sunt codificate (`OneHotEncoder`) cu un `ColumnTransformer`.\n",
    "\n",
    "3. **ÃmpÄƒrÈ›irea datasetului**:\n",
    "   - Setul este Ã®mpÄƒrÈ›it Ã®n subseturi de antrenare È™i testare (`train_test_split`), cu 80% pentru antrenare È™i 20% pentru test.\n",
    "\n",
    "4. **Crearea modelului**:\n",
    "   - ReÈ›ea neuronalÄƒ secvenÈ›ialÄƒ cu 2 straturi ascunse:\n",
    "     - 32 neuroni â†’ 16 neuroni â†’ 1 neuron de ieÈ™ire cu activare `sigmoid` (clasificare binarÄƒ).\n",
    "   - Optimizator: `adam`, funcÈ›ie de pierdere: `binary_crossentropy`.\n",
    "\n",
    "5. **Antrenare È™i evaluare**:\n",
    "   - Modelul este antrenat pe 80% din datele de antrenare, cu validare internÄƒ de 20%.\n",
    "   - Evaluarea se face pe setul de test (`evaluate()`), iar acurateÈ›ea este afiÈ™atÄƒ.\n",
    "\n",
    "Modelul de *reÈ›ea neuronalÄƒ* **oferÄƒ** o *acurateÈ›e competitivÄƒ* pe *setul de testare*. \n",
    "\n",
    "`Avantaje`:\n",
    "- **Capacitate de modelare** *complexÄƒ*\n",
    "- **Poate Ã®nvÄƒÈ›a** *relaÈ›ii subtile* Ã®n date\n",
    "\n",
    "`Dezavantaje`:\n",
    "- **NecesitÄƒ** *mai multÄƒ putere computaÈ›ionalÄƒ*\n",
    "- **Mai puÈ›in interpretabil** decÃ¢t modele simple\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
