{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94301caf",
   "metadata": {},
   "source": [
    "# 🩺 Predicția bolilor de inimă folosind PySpark, MLlib și TensorFlow\n",
    "\n",
    "## **Mihai Andrei-Alexandru**, *Big Data Project*, *iunie 2025*\n",
    "\n",
    "---\n",
    "---\n",
    "## Cuprins\n",
    "\n",
    "1. [Introducere](#1-introducere)\n",
    "2. [Procesarea datelor cu Spark](#2-procesarea-datelor-cu-spark)\n",
    "3. [Metode de Machine Learning](#3-metode-de-machine-learning)\n",
    "4. [Data Pipeline complet](#4-utilizarea-unui-data-pipeline-complet)\n",
    "5. [Functii definite de utilizator (UDF)](#5-utilizarea-unei-funcții-definite-de-utilizator-udf)\n",
    "6. [Deep Learning cu TensorFlow](#5-utilizarea-unei-funcții-definite-de-utilizator-udf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6e0aa",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 1. Introducere\n",
    "\n",
    "## Prezentarea succintă a setului de date\n",
    "\n",
    "Setul de date utilizat este **Heart Failure Prediction**, disponibil pe Kaggle:\n",
    "https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction\n",
    "\n",
    "Setul de date conține 918 înregistrări despre pacienți, fiecare cu 12 caracteristici medicale și o variabilă țintă `HeartDisease` care indică dacă pacientul **are sau nu boală cardiovasculară**.\n",
    "\n",
    "## Obiective\n",
    "\n",
    "Obiectivul proiectului este:\n",
    "- **procesarea**, **curățarea** și **analizarea** setului de date folosind *Spark*;\n",
    "- **antrenarea** și **evaluarea** a două modele *ML*;\n",
    "- **aplicarea** unei metode de *DL* cu *TensorFlow*;\n",
    "- **implementarea** unui *pipeline* și *UDF*;\n",
    "- **integrarea** unui *flux de date în timp real* cu *Spark Streaming*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19630cf",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 2. Procesarea datelor cu Spark\n",
    "\n",
    "Vom folosi *PySpark* pentru a **analiza** și **prelucra** datele. Se vor aplica **agregări** și **transformări** folosind atât *DataFrame API* cât și *Spark SQL*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7a2ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/15 09:56:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- ChestPainType: string (nullable = true)\n",
      " |-- RestingBP: integer (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- FastingBS: integer (nullable = true)\n",
      " |-- RestingECG: string (nullable = true)\n",
      " |-- MaxHR: integer (nullable = true)\n",
      " |-- ExerciseAngina: string (nullable = true)\n",
      " |-- Oldpeak: double (nullable = true)\n",
      " |-- ST_Slope: string (nullable = true)\n",
      " |-- HeartDisease: integer (nullable = true)\n",
      "\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "| 40|  M|          ATA|      140|        289|        0|    Normal|  172|             N|    0.0|      Up|           0|\n",
      "| 49|  F|          NAP|      160|        180|        0|    Normal|  156|             N|    1.0|    Flat|           1|\n",
      "| 37|  M|          ATA|      130|        283|        0|        ST|   98|             N|    0.0|      Up|           0|\n",
      "| 48|  F|          ASY|      138|        214|        0|    Normal|  108|             Y|    1.5|    Flat|           1|\n",
      "| 54|  M|          NAP|      150|        195|        0|    Normal|  122|             N|    0.0|      Up|           0|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HeartFailurePrediction\").getOrCreate()\n",
    "\n",
    "# Citirea setului de date\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"heart.csv\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "205b5459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/15 09:56:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "|summary|               Age| Sex|ChestPainType|         RestingBP|       Cholesterol|          FastingBS|RestingECG|             MaxHR|ExerciseAngina|           Oldpeak|ST_Slope|       HeartDisease|\n",
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "|  count|               918| 918|          918|               918|               918|                918|       918|               918|           918|               918|     918|                918|\n",
      "|   mean|53.510893246187365|NULL|         NULL|132.39651416122004| 198.7995642701525|0.23311546840958605|      NULL|136.80936819172112|          NULL|0.8873638344226581|    NULL| 0.5533769063180828|\n",
      "| stddev|  9.43261650673202|NULL|         NULL|18.514154119907808|109.38414455220345|0.42304562473930296|      NULL| 25.46033413825029|          NULL|1.0665701510493264|    NULL|0.49741373828459706|\n",
      "|    min|                28|   F|          ASY|                 0|                 0|                  0|       LVH|                60|             N|              -2.6|    Down|                  0|\n",
      "|    max|                77|   M|           TA|               200|               603|                  1|        ST|               202|             Y|               6.2|      Up|                  1|\n",
      "+-------+------------------+----+-------------+------------------+------------------+-------------------+----------+------------------+--------------+------------------+--------+-------------------+\n",
      "\n",
      "+------------+-----+\n",
      "|HeartDisease|count|\n",
      "+------------+-----+\n",
      "|           1|  508|\n",
      "|           0|  410|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistici descriptive\n",
    "df.describe().show()\n",
    "\n",
    "# Distribuția țintei (HeartDisease)\n",
    "df.groupBy(\"HeartDisease\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec757d",
   "metadata": {},
   "source": [
    "## Curățare și transformare date\n",
    "\n",
    "**Verificăm** *valori lipsă* și **realizăm transformări** simple: *conversia la lowercase*, *eliminarea duplicatelor*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfacaca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|  0|  0|            0|        0|          0|        0|         0|    0|             0|      0|       0|           0|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificare valori lipsă\n",
    "from pyspark.sql.functions import col, isnan, when, count, trim\n",
    "\n",
    "# Pentru fiecare coloana din set-ul nostru de date, numaram valorile egale cu NULL sau stringurile vide.\n",
    "df.select([\n",
    "    count(\n",
    "        when(\n",
    "            col(c).isNull() | (trim(col(c)) == \"\"), c\n",
    "        )\n",
    "    ).alias(c)\n",
    "    for c in df.columns\n",
    "]).show()\n",
    "\n",
    "# Eliminare duplicate (dacă există)\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c0cf9",
   "metadata": {},
   "source": [
    "Din fericire set-ul de date pe care l-am ales **nu a continut** *NULL* sau *stringuri vide*, dar acest lucru **nu este valabil** pentru orice dataset, deci acesta prelucrare **este obligatorie**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39a424",
   "metadata": {},
   "source": [
    "## Grupări și agregări cu DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e72a9",
   "metadata": {},
   "source": [
    "Agregare: procentul bolnavilor pe gen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0df7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|Sex|HeartDiseaseRatePercent|\n",
      "+---+-----------------------+\n",
      "|  F|                  25.91|\n",
      "|  M|                  63.17|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round, avg\n",
    "\n",
    "df.groupBy(\"Sex\") \\\n",
    "  .agg(round(avg(\"HeartDisease\") * 100, 2).alias(\"HeartDiseaseRatePercent\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d874e34",
   "metadata": {},
   "source": [
    "Grupare pe categorii de vârstă:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8fd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|AgeGroup|HeartDiseaseRatePercent|\n",
      "+--------+-----------------------+\n",
      "|   40-59|                  50.77|\n",
      "|     60+|                  73.12|\n",
      "|     <40|                   32.5|\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn(\"AgeGroup\", when(col(\"Age\") < 40, \"<40\")\n",
    "                              .when((col(\"Age\") >= 40) & (col(\"Age\") < 60), \"40-59\")\n",
    "                              .otherwise(\"60+\"))\n",
    "\n",
    "df.groupBy(\"AgeGroup\") \\\n",
    "  .agg(round(avg(\"HeartDisease\") * 100, 2).alias(\"HeartDiseaseRatePercent\")) \\\n",
    "  .orderBy(\"AgeGroup\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2192ffd",
   "metadata": {},
   "source": [
    "## Interogări cu Spark SQL\n",
    "\n",
    "Rata bolii în funcție de *glicemie* (**FastingBS**) și *tensiune arterială* (**RestingBP**):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e79c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----------------------+\n",
      "|Diabetic|HighBP|Total|HeartDiseaseRatePercent|\n",
      "+--------+------+-----+-----------------------+\n",
      "|       1|     0|   80|                   80.0|\n",
      "|       1|     1|  134|                   79.1|\n",
      "|       0|     1|  409|                  50.86|\n",
      "|       0|     0|  295|                  44.07|\n",
      "+--------+------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"HighBP\", when(col(\"RestingBP\") >= 130, 1).otherwise(0))\n",
    "df.createOrReplaceTempView(\"heart\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "       FastingBS AS Diabetic,\n",
    "       HighBP,\n",
    "       COUNT(*) AS Total,\n",
    "       ROUND(AVG(HeartDisease) * 100, 2) AS HeartDiseaseRatePercent\n",
    "    FROM heart\n",
    "    GROUP BY FastingBS, HighBP\n",
    "    ORDER BY HeartDiseaseRatePercent DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62bf99",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 3. Metode de Machine Learning\n",
    "\n",
    "Vom **construi** și **evalua** *două modele* de *clasificare binară* pentru a **prezice** apariția *bolii cardiace (`HeartDisease`)*:\n",
    "\n",
    "1. *Logistic Regression*\n",
    "2. *Random Forest Classifier*\n",
    "\n",
    "Scopul este să comparăm performanța celor două metode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4417276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+-----+\n",
      "|features                                            |label|\n",
      "+----------------------------------------------------+-----+\n",
      "|(11,[0,1,2,4,7,10],[40.0,140.0,289.0,172.0,2.0,1.0])|1.0  |\n",
      "|[49.0,160.0,180.0,0.0,156.0,1.0,1.0,1.0,0.0,0.0,0.0]|0.0  |\n",
      "|[37.0,130.0,283.0,0.0,98.0,0.0,0.0,2.0,2.0,0.0,1.0] |1.0  |\n",
      "|[48.0,138.0,214.0,0.0,108.0,1.5,1.0,0.0,0.0,1.0,0.0]|0.0  |\n",
      "|(11,[0,1,2,4,7,10],[54.0,150.0,195.0,122.0,1.0,1.0])|1.0  |\n",
      "+----------------------------------------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Selectăm coloanele numerice\n",
    "feature_cols = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "# Transformăm și coloanele categorice în numeric (dacă există)\n",
    "categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\") for col in categorical_cols]\n",
    "\n",
    "# Asamblare vector caracteristici\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols + [col + \"_idx\" for col in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Etichetă (HeartDisease este deja 0/1 dar asigurăm consistența)\n",
    "label_indexer = StringIndexer(inputCol=\"HeartDisease\", outputCol=\"label\")\n",
    "\n",
    "# Pipeline de preprocesare\n",
    "pipeline = Pipeline(stages=indexers + [assembler, label_indexer])\n",
    "data = pipeline.fit(df).transform(df).select(\"features\", \"label\")\n",
    "data.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd804f8",
   "metadata": {},
   "source": [
    "## Împărțirea datelor\n",
    "\n",
    "Vom împărți setul de date în două subseturi: antrenare (70%) și test (30%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a93d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 681, Test: 237\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "print(f\"Train: {train_data.count()}, Test: {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a80236",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- Vrem să **prezicem** dacă o persoană are *boală cardiacă* (*HeartDisease = 1*) pe baza unor *caracteristici clinice*.\n",
    "\n",
    "- **Logistic Regression** este o alegere bună pentru *clasificare binară*, deoarece este *interpretabil*, *rapid de antrenat* și oferă o primă linie de bază.\n",
    "\n",
    "- **Aplicăm** *modelul* și **evaluăm** *acuratețea* pe *setul de testare*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (Logistic Regression): 0.8969\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "lr_auc = evaluator.evaluate(lr_predictions)\n",
    "print(f\"AUC (Logistic Regression): {lr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9517a5",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "La fel ca mai sus, dorim să facem clasificare binară.\n",
    "\n",
    "Random Forest este un model de tip \"ensemble\" ce poate surprinde relații non-liniare între variabile. Este mai robust decât Logistic Regression, dar mai lent.\n",
    "\n",
    "Antrenăm un model RF și comparăm performanța sa cu Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f43b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (Random Forest): 0.9155\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, seed=42)\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "rf_auc = evaluator.evaluate(rf_predictions)\n",
    "print(f\"AUC (Random Forest): {rf_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44521f75",
   "metadata": {},
   "source": [
    "## Concluzii ML\n",
    "\n",
    "Pentru ambele modele am folosit *AUC* - (**Area Under Curve**). \n",
    "\n",
    "Aceasta este o *metrică standard* folosită pentru *evaluarea performanței* unui model de *clasificare binară*, în special când clasele sunt *dezechilibrate*.\n",
    "\n",
    "| Model               | AUC         |\n",
    "|--------------------|-------------|\n",
    "| *Logistic Regression*| ~ *0.8969* |\n",
    "| *Random Forest*      | ~ *0.9155* |\n",
    "\n",
    "*Random Forest* a oferit performanță mai bună datorită capacității sale de a **modela relații complexe** între trăsături. Totuși, *Logistic Regression* este **util** ca *model de bază* și pentru *interpretabilitate*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519008d",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 4. Utilizarea unui Data Pipeline complet\n",
    "\n",
    "Vom **construi** un *Pipeline* care include:\n",
    "\n",
    "- **Preprocesare**: *indexare categorice* + *VectorAssembler*\n",
    "- **Model ML**: *Logistic Regression*\n",
    "\n",
    "Ne dorim să automatizăm întreaga secvență de transformări și învățare automată.\n",
    "\n",
    "Etape: \n",
    "\n",
    "1. **Indexare categorică**: Coloanele *string* sunt transformate în coloane numerice folosind *StringIndexer*.\n",
    "\n",
    "2. **VectorAssembler**: Toate coloanele numerice și cele indexate sunt combinate într-un singur vector de trăsături (*features*) necesar pentru modele ML.\n",
    "\n",
    "3. **Label Indexing**: Coloana țintă *HeartDisease* este transformată în eticheta *label*.\n",
    "\n",
    "4. **Modelul**: Se utilizează *LogisticRegression* pentru clasificare binară.\n",
    "\n",
    "5. **Pipeline**: Toți pașii anteriori sunt legați într-un *Pipeline* unitar ce permite aplicarea secvențială a etapelor.\n",
    "\n",
    "6. **Antrenare & Predicție**: *fit()* antrenează pipeline-ul pe date, *transform()* aplică modelul pe același set.\n",
    "\n",
    "7. **Evaluare**: Se utilizează *BinaryClassificationEvaluator* cu metrica *areaUnderROC* pentru a evalua performanța modelului."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b8b8156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/15 09:57:34 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC cu pipeline complet (Logistic Regression): 0.9194\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Refacem pipeline-ul complet: indexeri + assembler + model\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\") for col in categorical_cols]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols + [col + \"_idx\" for col in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "label_indexer = StringIndexer(inputCol=\"HeartDisease\", outputCol=\"label\")\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Pipeline complet\n",
    "full_pipeline = Pipeline(stages=indexers + [assembler, label_indexer, lr])\n",
    "\n",
    "# Antrenare + predicție\n",
    "pipeline_model = full_pipeline.fit(df)\n",
    "predictions = pipeline_model.transform(df)\n",
    "\n",
    "# Evaluare rapidă\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "pipeline_auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC cu pipeline complet (Logistic Regression): {pipeline_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e917c",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 5. Utilizarea unei funcții definite de utilizator (UDF)\n",
    "\n",
    "Voi crea o funcție care calculează un scor de risc personalizat în funcție de vârstă și colesterol, urmând să adaug acest scor ca o coloană nouă pentru analiză sau input în model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "677270ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+\n",
      "|Age|Cholesterol|RiskScore|\n",
      "+---+-----------+---------+\n",
      "| 40|        289|   0.8225|\n",
      "| 49|        180|   0.5725|\n",
      "| 37|        283|      0.8|\n",
      "| 48|        214|    0.655|\n",
      "| 54|        195|   0.6225|\n",
      "+---+-----------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Funcție de calcul risc: rudimentară pentru demonstrație\n",
    "def risk_score(age, cholesterol):\n",
    "    score = 0.5 * age + 0.5 * (cholesterol if cholesterol else 0)\n",
    "    return float(score / 200)  # normalizare\n",
    "\n",
    "risk_udf = udf(risk_score, DoubleType())\n",
    "\n",
    "# Aplicare UDF\n",
    "df_with_risk = df.withColumn(\"RiskScore\", risk_udf(col(\"Age\"), col(\"Cholesterol\")))\n",
    "df_with_risk.select(\"Age\", \"Cholesterol\", \"RiskScore\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127762d8",
   "metadata": {},
   "source": [
    "## Optimizarea hiperparametrilor (Grid Search)\n",
    "\n",
    "Voi **aplica** un *Grid Search* și *Cross Validation* pentru *Logistic Regression*.\n",
    "\n",
    "Îmi doresc să optimizez hiperparametrii, astfel, trebuie să găsesc valoarea optimă pentru *regParam* și *elasticNetParam*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7802870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/15 10:13:50 WARN CacheManager: Asked to cache already cached data.\n",
      "25/06/15 10:13:50 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC cu Logistic Regression + param tuning: 0.9195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand']\n",
      "Bad pipe message: %s [b'ol: max-age=0\\r\\nsec-ch-ua: \"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"\\r\\nsec-ch-ua-mobile: ?0\\r\\n']\n",
      "Bad pipe message: %s [b'c-ch-ua-platform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) A', b'leWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\\r\\nAccept: text/html,application/xhtml+xml,app']\n",
      "Bad pipe message: %s [b'cation/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nSec-Fet', b'-Site: none\\r\\nSec-Fetch-Mode: navigate\\r\\nSec-Fetch-User: ?1\\r\\nSec-Fetch-Dest: document\\r\\nAccept-Encodi']\n",
      "Bad pipe message: %s [b'ol: max-age=0\\r\\nsec-ch-ua: \"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"\\r\\nsec-ch-ua-mobile: ?0\\r\\n']\n",
      "Bad pipe message: %s [b'c-ch-ua-platform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) A', b'leWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\\r\\nAccept: text/html,application/xhtml+xml,app']\n",
      "Bad pipe message: %s [b'cation/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nSec-Fet', b'-Site: cross-site\\r\\nSec-Fetch-Mode: navigate\\r\\nSec-Fetch-User: ?1\\r\\nSec-Fetch-Dest: document\\r\\nAccept-']\n",
      "Bad pipe message: %s [b'coding: gzip, deflate, br, zstd\\r\\nAccept-Language: en-US,en;q=0.9,ro;']\n",
      "Bad pipe message: %s [b'ol: max-age=0\\r\\nsec-ch-ua: \"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"\\r\\nsec-ch-ua-mobile: ?0\\r\\n']\n",
      "Bad pipe message: %s [b'c-ch-ua-platform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) A', b'leWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\\r\\nAccept: text/html,application/xhtml+xml,app']\n",
      "Bad pipe message: %s [b'cation/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nSec-Fet', b'-Site: cross-site\\r\\nSec-Fetch-Mode: navigate\\r\\nSec-Fetch-Dest: document\\r\\nAccept-Encoding: gzip, defl']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Pipeline de bază (doar cu Logistic Regression)\n",
    "pipeline_lr = Pipeline(stages=indexers + [assembler, label_indexer, lr])\n",
    "\n",
    "# Grid de parametri\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Execută căutarea\n",
    "cv_model = crossval.fit(df)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Afișăm AUC pe modelul optimizat\n",
    "cv_auc = evaluator.evaluate(best_model.transform(df))\n",
    "print(f\"AUC cu Logistic Regression + param tuning: {cv_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda3159",
   "metadata": {},
   "source": [
    "## Concluzii\n",
    "\n",
    "Am **integrat** într-un *pipeline complet* pașii de *preprocesare*, *antrenare* și *evaluare*. Am **utilizat**:\n",
    "\n",
    "- *UDF* pentru **definirea** unei *logici personalizate de scor de risc*\n",
    "- *Grid search* și *cross validation* pentru *optimizarea hiperparametrilor*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ad205",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 6. Deep Learning cu TensorFlow\n",
    "\n",
    "Dorim să construim un model de clasificare binară care prezice apariția unei boli cardiace (*HeartDisease = 0 sau 1*) folosind rețele neuronale.\n",
    "\n",
    "Rețelele neuronale artificiale pot capta relații complexe, neliniare între variabile, mai ales când există mai multe atribute implicate. În comparație cu Logistic Regression sau Random Forest, DL oferă flexibilitate mai mare pentru modelarea relațiilor nelineare.\n",
    "\n",
    "Vom folosi TensorFlow + Keras pentru a antrena o rețea neuronală simplă cu:\n",
    "\n",
    "- 2 straturi ascunse (dense)\n",
    "- Funcția de activare *ReLU*\n",
    "- Funcția de pierdere *binary_crossentropy*\n",
    "- Optimizator *Adam*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43d4abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Salvăm datele Spark într-un Pandas DataFrame pentru TensorFlow\n",
    "pandas_df = df.select(\n",
    "    \"Age\", \"Sex\", \"ChestPainType\", \"RestingBP\", \"Cholesterol\",\n",
    "    \"FastingBS\", \"RestingECG\", \"MaxHR\", \"ExerciseAngina\", \n",
    "    \"Oldpeak\", \"ST_Slope\", \"HeartDisease\"\n",
    ").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72493f",
   "metadata": {},
   "source": [
    "## Preprocesare date pentru TensorFlow\n",
    "\n",
    "Vom converti datele categorice în one-hot encoding și vom scala datele numerice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2804f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Separăm features și label\n",
    "X = pandas_df.drop(\"HeartDisease\", axis=1)\n",
    "y = pandas_df[\"HeartDisease\"]\n",
    "\n",
    "# Coloane categorice și numerice\n",
    "cat_cols = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"]\n",
    "num_cols = [\"Age\", \"RestingBP\", \"Cholesterol\", \"FastingBS\", \"MaxHR\", \"Oldpeak\"]\n",
    "\n",
    "# Pipeline de transformare\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(), cat_cols)\n",
    "])\n",
    "\n",
    "# Aplicăm transformările\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Împărțire în train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a424e0",
   "metadata": {},
   "source": [
    "## Construirea modelului TensorFlow\n",
    "\n",
    "Model secvențial cu 2 straturi ascunse și un strat final cu sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d383f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 09:58:04.517672: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-15 09:58:04.518433: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-15 09:58:04.523461: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-15 09:58:04.532728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749981484.549408     952 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749981484.554505     952 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749981484.568485     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749981484.568501     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749981484.568502     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749981484.568503     952 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-15 09:58:04.572540: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-06-15 09:58:05.888882: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,217</span> (4.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,217\u001b[0m (4.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,217</span> (4.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,217\u001b[0m (4.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # output pentru clasificare binară\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7104ec",
   "metadata": {},
   "source": [
    "## Antrenarea modelului\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c6f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77382587",
   "metadata": {},
   "source": [
    "## Evaluarea modelului pe setul de testare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8468 - loss: 0.3631 \n",
      "Test Accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23991ca7",
   "metadata": {},
   "source": [
    "## Concluzie Deep Learning\n",
    "\n",
    "În această secțiune am folosit **TensorFlow** pentru a construi și antrena un model de rețea neuronală care prezice probabilitatea de boală cardiacă pe baza caracteristicilor pacienților. Pașii principali sunt:\n",
    "\n",
    "1. **Conversie Spark → Pandas**:\n",
    "   - Datele Spark sunt convertite în format Pandas (`toPandas()`) pentru a fi compatibile cu TensorFlow și Scikit-learn.\n",
    "\n",
    "2. **Preprocesare date**:\n",
    "   - Separăm `X` (features) și `y` (eticheta).\n",
    "   - Coloanele numerice sunt scalate (`StandardScaler`) iar cele categorice sunt codificate (`OneHotEncoder`) cu un `ColumnTransformer`.\n",
    "\n",
    "3. **Împărțirea datasetului**:\n",
    "   - Setul este împărțit în subseturi de antrenare și testare (`train_test_split`), cu 80% pentru antrenare și 20% pentru test.\n",
    "\n",
    "4. **Crearea modelului**:\n",
    "   - Rețea neuronală secvențială cu 2 straturi ascunse:\n",
    "     - 32 neuroni → 16 neuroni → 1 neuron de ieșire cu activare `sigmoid` (clasificare binară).\n",
    "   - Optimizator: `adam`, funcție de pierdere: `binary_crossentropy`.\n",
    "\n",
    "5. **Antrenare și evaluare**:\n",
    "   - Modelul este antrenat pe 80% din datele de antrenare, cu validare internă de 20%.\n",
    "   - Evaluarea se face pe setul de test (`evaluate()`), iar acuratețea este afișată.\n",
    "\n",
    "Modelul de *rețea neuronală* **oferă** o *acuratețe competitivă* pe *setul de testare*. \n",
    "\n",
    "`Avantaje`:\n",
    "- **Capacitate de modelare** *complexă*\n",
    "- **Poate învăța** *relații subtile* în date\n",
    "\n",
    "`Dezavantaje`:\n",
    "- **Necesită** *mai multă putere computațională*\n",
    "- **Mai puțin interpretabil** decât modele simple\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
